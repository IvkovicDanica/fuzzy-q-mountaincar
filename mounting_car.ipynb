{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2513d1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import random\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from skfuzzy.membership import trapmf\n",
    "from typing import List, Tuple\n",
    "import copy\n",
    "from helper import explain_rule_strengths, FQLModelVerbose\n",
    "\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78192e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Andrija Lukic\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pygame\\pkgdata.py:27: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"MountainCar-v0\", render_mode=\"human\", goal_velocity=0.1)\n",
    "\n",
    "observation, info = env.reset(seed=42)\n",
    "for _ in range(1000):\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    \n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b6134b",
   "metadata": {},
   "source": [
    "## Mathematical Explanation\n",
    "\n",
    "### 1. Trapezoidal Membership Function\n",
    "\n",
    "A trapezoidal membership function is defined by four parameters:\n",
    "\n",
    "$$\n",
    "\\text{Trapezium}(a, b, c, d)\n",
    "$$\n",
    "\n",
    "Where:  \n",
    "- $a$ = left (start of slope-up)  \n",
    "- $b$ = left\\_top (start of plateau)  \n",
    "- $c$ = right\\_top (end of plateau)  \n",
    "- $d$ = right (end of slope-down)  \n",
    "\n",
    "The membership function $\\mu(x)$ is given by:\n",
    "\n",
    "$$\n",
    "\\mu(x) =\n",
    "\\begin{cases}\n",
    "0, & x \\le a \\\\\n",
    "\\frac{x - a}{b - a}, & a < x < b \\\\\n",
    "1, & b \\le x \\le c \\\\\n",
    "\\frac{d - x}{d - c}, & c < x < d \\\\\n",
    "0, & x \\ge d\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "### 2. Membership Values for an Input Variable\n",
    "\n",
    "For a crisp input value $x$ and a set of $n$ fuzzy sets $F_1, F_2, \\dots, F_n$,  \n",
    "the membership vector is:\n",
    "\n",
    "$$\n",
    "M(x) = \\left[ \\mu_{F_1}(x), \\mu_{F_2}(x), \\dots, \\mu_{F_n}(x) \\right]\n",
    "$$\n",
    "\n",
    "### 3. Rule Membership Calculation\n",
    "\n",
    "If the system has $k$ input variables, each with $n_i$ fuzzy sets,  \n",
    "the total number of rules is:\n",
    "\n",
    "$$\n",
    "N_{\\text{rules}} = \\prod_{i=1}^k n_i\n",
    "$$\n",
    "\n",
    "For a given crisp state:\n",
    "\n",
    "$$\n",
    "S = [x_1, x_2, \\dots, x_k]\n",
    "$$\n",
    "\n",
    "Each rule corresponds to a combination of fuzzy set indices:\n",
    "\n",
    "$$\n",
    "R_j = (f_1, f_2, \\dots, f_k), \\quad f_i \\in \\{1, \\dots, n_i\\}\n",
    "$$\n",
    "\n",
    "The unnormalized membership of a rule is the product of the membership degrees:\n",
    "\n",
    "$$\n",
    "\\mu_{R_j} = \\prod_{i=1}^k \\mu_{F_{i, f_i}}(x_i)\n",
    "$$\n",
    "\n",
    "### 4. Normalization of Rule Memberships\n",
    "\n",
    "Finally, rule memberships are normalized to sum to 1:\n",
    "\n",
    "$$\n",
    "\\mu_{R_j}^{\\text{norm}} = \\frac{\\mu_{R_j}}{\\sum_{m=1}^{N_{\\text{rules}}} \\mu_{R_m}}\n",
    "$$\n",
    "\n",
    "This ensures the set of rule memberships forms a probability-like distribution over all rules.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b03d3d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Trapezium:\n",
    "    \"\"\"\n",
    "    Represents a trapezoidal membership function for fuzzy logic.\n",
    "    \n",
    "    Attributes:\n",
    "        left (float): Start of the trapezoid's base (lower bound).\n",
    "        left_top (float): Start of the top plateau.\n",
    "        right_top (float): End of the top plateau.\n",
    "        right (float): End of the trapezoid's base (upper bound).\n",
    "    \"\"\"\n",
    "    left: float\n",
    "    left_top: float\n",
    "    right_top: float\n",
    "    right: float\n",
    "\n",
    "    def membership_value(self, input_value: float) -> float:\n",
    "        \"\"\"\n",
    "        Calculate the membership value of an input for this trapezoidal fuzzy set.\n",
    "\n",
    "        Args:\n",
    "            input_value (float): The crisp input value.\n",
    "\n",
    "        Returns:\n",
    "            float: Membership value in the range [0, 1].\n",
    "        \"\"\"\n",
    "        x = np.array([input_value])\n",
    "        params = [self.left, self.left_top, self.right_top, self.right]\n",
    "        return float(trapmf(x, params)[0])\n",
    "\n",
    "\n",
    "class InputStateVariable:\n",
    "    \"\"\"\n",
    "    Represents a fuzzy input variable containing multiple fuzzy sets.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *fuzzy_sets: Trapezium):\n",
    "        \"\"\"\n",
    "        Initialize an input variable with fuzzy sets.\n",
    "\n",
    "        Args:\n",
    "            *fuzzy_sets (Trapezium): One or more trapezoidal fuzzy sets.\n",
    "        \"\"\"\n",
    "        self.fuzzy_set_list: Tuple[Trapezium, ...] = fuzzy_sets\n",
    "\n",
    "    def get_fuzzy_sets(self) -> Tuple[Trapezium, ...]:\n",
    "        \"\"\"\n",
    "        Get all fuzzy sets for this input variable.\n",
    "\n",
    "        Returns:\n",
    "            tuple[Trapezium, ...]: The fuzzy sets.\n",
    "        \"\"\"\n",
    "        return self.fuzzy_set_list\n",
    "\n",
    "    def get_memberships(self, value: float) -> List[float]:\n",
    "        \"\"\"\n",
    "        Get membership values for a crisp value across all fuzzy sets.\n",
    "\n",
    "        Args:\n",
    "            value (float): The crisp input value.\n",
    "\n",
    "        Returns:\n",
    "            list[float]: Membership values for each fuzzy set.\n",
    "        \"\"\"\n",
    "        return [fs.membership_value(value) for fs in self.fuzzy_set_list]\n",
    "\n",
    "\n",
    "class Build:\n",
    "    \"\"\"\n",
    "    Represents a fuzzy inference system builder.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *input_vars: InputStateVariable):\n",
    "        \"\"\"\n",
    "        Initialize the fuzzy system with input variables.\n",
    "\n",
    "        Args:\n",
    "            *input_vars (InputStateVariable): One or more input variables.\n",
    "        \"\"\"\n",
    "        self.input_vars: Tuple[InputStateVariable, ...] = input_vars\n",
    "\n",
    "    def get_input(self) -> Tuple[InputStateVariable, ...]:\n",
    "        \"\"\"\n",
    "        Get all input variables.\n",
    "\n",
    "        Returns:\n",
    "            tuple[InputStateVariable, ...]: The input variables.\n",
    "        \"\"\"\n",
    "        return self.input_vars\n",
    "\n",
    "    def get_number_of_fuzzy_sets(self, input_variable: InputStateVariable) -> int:\n",
    "        \"\"\"\n",
    "        Get the number of fuzzy sets for a given input variable.\n",
    "\n",
    "        Args:\n",
    "            input_variable (InputStateVariable): The input variable.\n",
    "\n",
    "        Returns:\n",
    "            int: Number of fuzzy sets.\n",
    "        \"\"\"\n",
    "        return len(input_variable.get_fuzzy_sets())\n",
    "\n",
    "    def get_number_of_rules(self) -> int:\n",
    "        \"\"\"\n",
    "        Compute the total number of fuzzy rules.\n",
    "\n",
    "        Returns:\n",
    "            int: The number of possible rules.\n",
    "        \"\"\"\n",
    "        num_rules = 1\n",
    "        for var in self.input_vars:\n",
    "            num_rules *= self.get_number_of_fuzzy_sets(var)\n",
    "        return num_rules\n",
    "\n",
    "    def get_rule_memberships(self, state: List[float]) -> List[float]:\n",
    "        \"\"\"\n",
    "        Calculate normalized membership values for all possible rules.\n",
    "\n",
    "        Args:\n",
    "            state (list[float]): Crisp values for each input variable.\n",
    "\n",
    "        Returns:\n",
    "            list[float]: Normalized rule membership degrees.\n",
    "        \"\"\"\n",
    "        memberships = [\n",
    "            var.get_memberships(state[i]) for i, var in enumerate(self.input_vars)\n",
    "        ]\n",
    "\n",
    "        rule_memberships = []\n",
    "        for idx in np.ndindex(*[len(m) for m in memberships]):\n",
    "            mu = 1.0\n",
    "            for var_idx, set_idx in enumerate(idx):\n",
    "                mu *= memberships[var_idx][set_idx]\n",
    "            rule_memberships.append(mu)\n",
    "\n",
    "        total = sum(rule_memberships)\n",
    "        if total > 0:\n",
    "            rule_memberships = [m / total for m in rule_memberships]\n",
    "\n",
    "        return rule_memberships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a11e05f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "                   FUZZY INFERENCE — Step-by-step evaluation                    \n",
      "================================================================================\n",
      "\n",
      "1) Crisp input values:\n",
      "   Input  1: value = 18\n",
      "   Input  2: value = 35\n",
      "\n",
      "2) Memberships per input variable (for the given crisp values):\n",
      "\n",
      "   Input 1 (value = 18):\n",
      "      1. Low          | params = (0, 0, 10, 20) -> μ = 0.2000\n",
      "      2. High         | params = (15, 25, 35, 35) -> μ = 0.3000\n",
      "\n",
      "   Input 2 (value = 35):\n",
      "      1. Slow         | params = (0, 0, 20, 40) -> μ = 0.2500\n",
      "      2. Fast         | params = (30, 50, 70, 70) -> μ = 0.2500\n",
      "\n",
      "3) Building rule combinations and computing unnormalized strengths:\n",
      "\n",
      "   Rule | Antecedents                    | Memberships          |     Unnorm\n",
      "   ----------------------------------------------------------------------\n",
      "      1 | Low, Slow                      | 0.2000, 0.2500       |     0.0500\n",
      "      2 | Low, Fast                      | 0.2000, 0.2500       |     0.0500\n",
      "      3 | High, Slow                     | 0.3000, 0.2500       |     0.0750\n",
      "      4 | High, Fast                     | 0.3000, 0.2500       |     0.0750\n",
      "\n",
      "4) Normalization:\n",
      "   Sum of unnormalized strengths = 0.2500\n",
      "\n",
      "   Rule | Antecedents                    | Memberships          |     Unnorm |       Norm\n",
      "   ------------------------------------------------------------------------------------------\n",
      "      1 | Low, Slow                      | 0.2000, 0.2500       |     0.0500 |     0.2000\n",
      "      2 | Low, Fast                      | 0.2000, 0.2500       |     0.0500 |     0.2000\n",
      "      3 | High, Slow                     | 0.3000, 0.2500       |     0.0750 |     0.3000\n",
      "      4 | High, Fast                     | 0.3000, 0.2500       |     0.0750 |     0.3000\n",
      "\n",
      "5) Final rule mapping (sorted by normalized strength desc):\n",
      "    1. Rule 3: IF High AND Slow => normalized strength = 0.3000\n",
      "    2. Rule 4: IF High AND Fast => normalized strength = 0.3000\n",
      "    3. Rule 1: IF Low AND Slow => normalized strength = 0.2000\n",
      "    4. Rule 2: IF Low AND Fast => normalized strength = 0.2000\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "temperature = InputStateVariable(\n",
    "    Trapezium(0, 0, 10, 20),   # Low\n",
    "    Trapezium(15, 25, 35, 35)  # High\n",
    ")\n",
    "\n",
    "speed = InputStateVariable(\n",
    "    Trapezium(0, 0, 20, 40),   # Slow\n",
    "    Trapezium(30, 50, 70, 70)  # Fast\n",
    ")\n",
    "\n",
    "system = Build(temperature, speed)\n",
    "state = [18, 35]\n",
    "labels = [[\"Low\", \"High\"], [\"Slow\", \"Fast\"]]\n",
    "\n",
    "explain_rule_strengths(system, state, labels=labels, decimals=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1182afb",
   "metadata": {},
   "source": [
    "# Fuzzy Q-Learning — Mathematical Explanation\n",
    "\n",
    "## Notation\n",
    "- Let there be $k$ input variables (e.g., Temperature, Speed).\n",
    "- Input variable $i$ has $n_i$ fuzzy sets.\n",
    "- Each fuzzy set is a trapezoid $\\mathrm{Trapezium}(a,b,c,d)$.\n",
    "- A *rule* $R_j$ is a particular combination of one fuzzy set from each input.\n",
    "- The total number of rules:\n",
    "  $$\n",
    "  N_{\\text{rules}} = \\prod_{i=1}^k n_i.\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Trapezoidal membership function\n",
    "A trapezoid is parameterised by $a,b,c,d$. The membership of crisp $x$ in that trapezoid is:\n",
    "\n",
    "$$\n",
    "\\mu(x) =\n",
    "\\begin{cases}\n",
    "0, & x \\le a \\\\\n",
    "\\dfrac{x - a}{b - a}, & a < x < b \\\\\n",
    "1, & b \\le x \\le c \\\\\n",
    "\\dfrac{d - x}{d - c}, & c < x < d \\\\\n",
    "0, & x \\ge d\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Per-variable membership vector\n",
    "For a given input variable $i$ and crisp value $x_i$, compute memberships across its fuzzy sets:\n",
    "\n",
    "$$\n",
    "M_i(x_i) = \\big[ \\mu_{i,1}(x_i), \\mu_{i,2}(x_i), \\dots, \\mu_{i,n_i}(x_i)\\big].\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Rule (antecedent) unnormalized strength\n",
    "For rule $R_j$ corresponding to fuzzy-set indices $(f_1,\\dots,f_k)$ (one index per input):\n",
    "\n",
    "$$\n",
    "\\tilde{\\mu}_{R_j} = \\prod_{i=1}^k \\mu_{i,f_i}(x_i).\n",
    "$$\n",
    "\n",
    "This is the product t-norm (AND) across inputs.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Normalize rule strengths\n",
    "Normalize rule strengths to sum to 1 (so they behave like a distribution over rules):\n",
    "\n",
    "$$\n",
    "\\mu_{R_j} = \\frac{\\tilde{\\mu}_{R_j}}{\\sum_{m=1}^{N_{\\text{rules}}} \\tilde{\\mu}_{R_m}}.\n",
    "$$\n",
    "\n",
    "Let the normalized rule-strength vector be $ \\mathbf{R} = [\\mu_{R_1}, \\dots, \\mu_{R_N}]. $\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Per-rule action choices\n",
    "For each rule $j$ we pick an action index $M_j$ (from $\\{0,\\dots,A-1\\}$) using an ε-greedy policy on that rule's Q-row:\n",
    "\n",
    "- with probability $\\epsilon$: choose a random action.\n",
    "- otherwise: choose $\\arg\\max_{a} Q[j,a]$.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Aggregate action selection (final action)\n",
    "Compute action weights by combining rule strengths with per-rule Q-values:\n",
    "\n",
    "$$\n",
    "W(a) = \\sum_{j=1}^{N} \\mu_{R_j} \\cdot Q[j,a]\n",
    "$$\n",
    "\n",
    "Pick final action:\n",
    "\n",
    "$$\n",
    "a^\\star = \\arg\\max_a W(a)\n",
    "$$\n",
    "\n",
    "(Optionally add a tiny random jitter to $W$ if the standard deviation is too small, to break ties.)\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Q-value for the *previous* state (scalar)\n",
    "When the agent previously observed state $s_{t-1}$ it recorded:\n",
    "- the rule strengths $\\mathbf{R}^{t-1}$, and\n",
    "- the per-rule chosen actions $M^{t-1}_j$.\n",
    "\n",
    "The scalar (aggregated) Q-value used as baseline is:\n",
    "\n",
    "$$\n",
    "Q_{\\text{prev}} = \\sum_{j=1}^{N} \\mu^{t-1}_{R_j} \\cdot Q\\big[j, M^{t-1}_j\\big].\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 8. State value for the *current* state\n",
    "For current state $s_t$ with rule strengths $\\mathbf{R}^t$, compute\n",
    "\n",
    "$$\n",
    "V_t = \\sum_{j=1}^{N} \\mu_{R_j}^t \\cdot \\max_a Q[j,a].\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Temporal-difference (TD) error and Q update\n",
    "Given immediate reward $r_t$ (received for the transition from $s_{t-1}$ to $s_t$), discount $\\gamma$ and learning rate $\\alpha$:\n",
    "\n",
    "TD error:\n",
    "\n",
    "$$\n",
    "\\delta_t = r_t + \\gamma \\, V_t \\;-\\; Q_{\\text{prev}}.\n",
    "$$\n",
    "\n",
    "Update the Q-table for each rule $j$ that was active in the previous state ($\\mu^{t-1}_{R_j} > 0$):\n",
    "\n",
    "$$\n",
    "Q[j, M^{t-1}_j] \\leftarrow Q[j, M^{t-1}_j] \\;+\\; \\alpha \\cdot \\delta_t \\cdot \\mu^{t-1}_{R_j}.\n",
    "$$\n",
    "\n",
    "This multiplies the TD error by the rule's activation in the previous state (responsibility weighting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236a2d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FQLModel:\n",
    "    \"\"\"\n",
    "    Fuzzy Q-Learning Model.\n",
    "\n",
    "    This class implements a fuzzy reinforcement learning agent using a Q-learning\n",
    "    approach with a fuzzy inference system (FIS) for state representation.\n",
    "\n",
    "    Attributes:\n",
    "        gamma (float): Discount factor for future rewards.\n",
    "        alpha (float): Learning rate.\n",
    "        epsilon (float): Exploration rate (for ε-greedy policy).\n",
    "        action_set_length (int): Number of possible actions.\n",
    "        fis (Build): Fuzzy inference system for computing rule memberships.\n",
    "        q_table (np.ndarray): Q-values table, shape = (num_rules, num_actions).\n",
    "        R (List[float]): Truth values (rule activations) for the current state.\n",
    "        R_ (List[float]): Truth values for the previous state.\n",
    "        M (List[int]): Selected action index per rule.\n",
    "        V (List[float]): State value history.\n",
    "        Q (List[float]): Q-value history.\n",
    "        Error (float): Temporal Difference (TD) error.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, gamma: float, alpha: float, epsilon: float, action_set_length: int, fis: \"Build\"):\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        self.action_set_length = action_set_length\n",
    "        self.fis = fis\n",
    "\n",
    "        # Initialize Q-table: rows = rules, columns = actions\n",
    "        self.q_table = np.zeros((self.fis.get_number_of_rules(), action_set_length))\n",
    "\n",
    "        # Internal state variables\n",
    "        self.R: List[float] = []\n",
    "        self.R_: List[float] = []\n",
    "        self.M: List[int] = []\n",
    "        self.V: List[float] = []\n",
    "        self.Q: List[float] = []\n",
    "        self.Error: float = 0.0\n",
    "\n",
    "    def truth_value(self, state_value: List[float]) -> \"FQLModel\":\n",
    "        \"\"\"\n",
    "        Compute truth values (rule activations) for a given state.\n",
    "\n",
    "        Args:\n",
    "            state_value (List[float]): Crisp state values for each input variable.\n",
    "\n",
    "        Returns:\n",
    "            FQLModel: Self (for method chaining).\n",
    "        \"\"\"\n",
    "        self.R = self.fis.get_rule_memberships(state_value)\n",
    "        return self\n",
    "\n",
    "    def action_selection(self) -> int:\n",
    "        \"\"\"\n",
    "        Select an action using an ε-greedy strategy across fuzzy rules.\n",
    "\n",
    "        Returns:\n",
    "            int: Index of the selected action.\n",
    "        \"\"\"\n",
    "        self.M.clear()\n",
    "\n",
    "        # Select action for each rule\n",
    "        for rule_idx in range(len(self.R)):\n",
    "            if random.random() < self.epsilon:\n",
    "                # Exploration\n",
    "                action_index = random.randint(0, self.action_set_length - 1)\n",
    "            else:\n",
    "                # Exploitation\n",
    "                action_index = int(np.argmax(self.q_table[rule_idx]))\n",
    "            self.M.append(action_index)\n",
    "\n",
    "        # Aggregate actions weighted by rule activations\n",
    "        action_weights = np.zeros(self.action_set_length)\n",
    "        for rule_idx, truth_value in enumerate(self.R):\n",
    "            if truth_value > 0:\n",
    "                for action_idx in range(self.action_set_length):\n",
    "                    action_weights[action_idx] += truth_value * self.q_table[rule_idx, action_idx]\n",
    "\n",
    "        # Add small noise if actions are too similar\n",
    "        if np.std(action_weights) < 0.1:\n",
    "            action_weights += np.random.normal(0, 0.1, self.action_set_length)\n",
    "\n",
    "        return int(np.argmax(action_weights))\n",
    "\n",
    "    def calculate_q_value(self):\n",
    "        \"\"\"\n",
    "        Compute the Q-value for the previous state based on selected actions.\n",
    "        \"\"\"\n",
    "        q_curr = sum(\n",
    "            truth_value * self.q_table[index, self.M[index]]\n",
    "            for index, truth_value in enumerate(self.R_)\n",
    "        )\n",
    "        self.Q.append(q_curr)\n",
    "\n",
    "    def calculate_state_value(self):\n",
    "        \"\"\"\n",
    "        Compute the state value for the current state (max-Q over all actions for each rule).\n",
    "        \"\"\"\n",
    "        v_curr = sum(\n",
    "            self.R[index] * np.max(rule_q_values)\n",
    "            for index, rule_q_values in enumerate(self.q_table)\n",
    "        )\n",
    "        self.V.append(v_curr)\n",
    "\n",
    "    def update_q_value(self, reward: float) -> \"FQLModel\":\n",
    "        \"\"\"\n",
    "        Update the Q-table using the Temporal Difference (TD) learning rule.\n",
    "\n",
    "        Args:\n",
    "            reward (float): Immediate reward received.\n",
    "\n",
    "        Returns:\n",
    "            FQLModel: Self (for method chaining).\n",
    "        \"\"\"\n",
    "        if not self.V or not self.Q:\n",
    "            return self\n",
    "\n",
    "        # TD Error\n",
    "        self.Error = reward + self.gamma * self.V[-1] - self.Q[-1]\n",
    "\n",
    "        # Update Q-values for rules activated in previous state\n",
    "        for index, truth_value in enumerate(self.R_):\n",
    "            if truth_value > 0:\n",
    "                self.q_table[index, self.M[index]] += self.alpha * (self.Error * truth_value)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def save_state_history(self):\n",
    "        \"\"\"\n",
    "        Save current truth values (R) to R_ for the next update step.\n",
    "        \"\"\"\n",
    "        self.R_ = copy.copy(self.R)\n",
    "\n",
    "    def get_initial_action(self, state: List[float]) -> int:\n",
    "        \"\"\"\n",
    "        Get the first action for an episode, clearing history.\n",
    "\n",
    "        Args:\n",
    "            state (List[float]): Initial crisp state values.\n",
    "\n",
    "        Returns:\n",
    "            int: Selected action index.\n",
    "        \"\"\"\n",
    "        self.V.clear()\n",
    "        self.Q.clear()\n",
    "        self.truth_value(state)\n",
    "        action = self.action_selection()\n",
    "        self.calculate_q_value()\n",
    "        self.save_state_history()\n",
    "        return action\n",
    "\n",
    "    def get_action(self, state: List[float]) -> int:\n",
    "        \"\"\"\n",
    "        Select an action for the given state (no Q-update).\n",
    "\n",
    "        Args:\n",
    "            state (List[float]): Crisp state values.\n",
    "\n",
    "        Returns:\n",
    "            int: Selected action index.\n",
    "        \"\"\"\n",
    "        self.truth_value(state)\n",
    "        return self.action_selection()\n",
    "\n",
    "    def run(self, state: List[float], reward: float) -> int:\n",
    "        \"\"\"\n",
    "        Perform one step of the fuzzy Q-learning algorithm.\n",
    "\n",
    "        Args:\n",
    "            state (List[float]): Current crisp state values.\n",
    "            reward (float): Immediate reward received.\n",
    "\n",
    "        Returns:\n",
    "            int: Selected action index for the next step.\n",
    "        \"\"\"\n",
    "        self.truth_value(state)\n",
    "        self.calculate_state_value()\n",
    "        self.update_q_value(reward)\n",
    "        action = self.action_selection()\n",
    "        self.calculate_q_value()\n",
    "        self.save_state_history()\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963bfd7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== INITIAL ACTION SELECTION ===\n",
      "================================================================================\n",
      "STATE (t): [18, 35]\n",
      "\n",
      "Per-variable memberships:\n",
      "  Input 1: S1:0.2000, S2:0.3000\n",
      "  Input 2: S1:0.2500, S2:0.2500\n",
      "\n",
      "Normalized rule strengths (R_t):\n",
      "  Rule  1: R_t = 0.2000\n",
      "  Rule  2: R_t = 0.2000\n",
      "  Rule  3: R_t = 0.3000\n",
      "  Rule  4: R_t = 0.3000\n",
      "\n",
      "State value V_t = sum_j R_t[j] * max_a Q[j,a] = 0.0000\n",
      "Aggregated Q_prev (for previous state): 0.0000\n",
      "Received reward r_t = 0.0000\n",
      "\n",
      "TD error δ = r_t + γ * V_t - Q_prev = 0.0000\n",
      "\n",
      "No previous rule activations recorded — skipping Q update.\n",
      "\n",
      "Per-rule action choices for current state (ε-greedy per rule):\n",
      "  Rule  1: chosen action = 0 (exploit)\n",
      "  Rule  2: chosen action = 1 (explore)\n",
      "  Rule  3: chosen action = 0 (exploit)\n",
      "  Rule  4: chosen action = 0 (explore)\n",
      "\n",
      "Action weights had nearly zero variance, added tiny jitter to break ties.\n",
      "\n",
      "Aggregate action weights (W[a] = sum_j R_t[j] * Q[j,a]):\n",
      "  Action 0: W = 0.0050\n",
      "  Action 1: W = -0.0014\n",
      "  Action 2: W = 0.0065\n",
      "\n",
      "Selected final action: 2\n",
      "================================================================================\n",
      "\n",
      "Initial chosen action: 2\n",
      "\n",
      "\n",
      "================================================================================\n",
      "STATE (t): [23.684482059801894, 64.60897838524227]\n",
      "\n",
      "Per-variable memberships:\n",
      "  Input 1: S1:0.0000, S2:0.8684\n",
      "  Input 2: S1:0.0000, S2:1.0000\n",
      "\n",
      "Normalized rule strengths (R_t):\n",
      "  Rule  1: R_t = 0.0000\n",
      "  Rule  2: R_t = 0.0000\n",
      "  Rule  3: R_t = 0.0000\n",
      "  Rule  4: R_t = 1.0000\n",
      "\n",
      "State value V_t = sum_j R_t[j] * max_a Q[j,a] = 0.0000\n",
      "Aggregated Q_prev (for previous state): 0.0000\n",
      "Received reward r_t = -74.0000\n",
      "\n",
      "TD error δ = r_t + γ * V_t - Q_prev = -74.0000\n",
      "\n",
      "Updating Q-table for rules active in previous state (showing changed entries):\n",
      "  Rule  1, action 0: Q_before=0.0000 + (α*δ*R_prev)=-1.4800 -> Q_after=-1.4800\n",
      "  Rule  2, action 1: Q_before=0.0000 + (α*δ*R_prev)=-1.4800 -> Q_after=-1.4800\n",
      "  Rule  3, action 0: Q_before=0.0000 + (α*δ*R_prev)=-2.2200 -> Q_after=-2.2200\n",
      "  Rule  4, action 0: Q_before=0.0000 + (α*δ*R_prev)=-2.2200 -> Q_after=-2.2200\n",
      "\n",
      "Per-rule action choices for current state (ε-greedy per rule):\n",
      "  Rule  1: chosen action = 1 (explore)\n",
      "  Rule  2: chosen action = 0 (explore)\n",
      "  Rule  3: chosen action = 1 (exploit)\n",
      "  Rule  4: chosen action = 1 (exploit)\n",
      "\n",
      "Aggregate action weights (W[a] = sum_j R_t[j] * Q[j,a]):\n",
      "  Action 0: W = -2.2200\n",
      "  Action 1: W = 0.0000\n",
      "  Action 2: W = 0.0000\n",
      "\n",
      "Selected final action: 1\n",
      "================================================================================\n",
      "\n",
      "Step  1: new state = [23.684482059801894, 64.60897838524227], chosen action = 1, reward w.r.t prev state = -74.00\n",
      "\n",
      "================================================================================\n",
      "STATE (t): [0.9287589389352269, 29.941882534332425]\n",
      "\n",
      "Per-variable memberships:\n",
      "  Input 1: S1:1.0000, S2:0.0000\n",
      "  Input 2: S1:0.5029, S2:0.0000\n",
      "\n",
      "Normalized rule strengths (R_t):\n",
      "  Rule  1: R_t = 1.0000\n",
      "  Rule  2: R_t = 0.0000\n",
      "  Rule  3: R_t = 0.0000\n",
      "  Rule  4: R_t = 0.0000\n",
      "\n",
      "State value V_t = sum_j R_t[j] * max_a Q[j,a] = 0.0000\n",
      "Aggregated Q_prev (for previous state): 0.0000\n",
      "Received reward r_t = -607.3324\n",
      "\n",
      "TD error δ = r_t + γ * V_t - Q_prev = -607.3324\n",
      "\n",
      "Updating Q-table for rules active in previous state (showing changed entries):\n",
      "  Rule  4, action 1: Q_before=0.0000 + (α*δ*R_prev)=-60.7332 -> Q_after=-60.7332\n",
      "\n",
      "Per-rule action choices for current state (ε-greedy per rule):\n",
      "  Rule  1: chosen action = 1 (exploit)\n",
      "  Rule  2: chosen action = 0 (exploit)\n",
      "  Rule  3: chosen action = 1 (exploit)\n",
      "  Rule  4: chosen action = 2 (exploit)\n",
      "\n",
      "Aggregate action weights (W[a] = sum_j R_t[j] * Q[j,a]):\n",
      "  Action 0: W = -1.4800\n",
      "  Action 1: W = 0.0000\n",
      "  Action 2: W = 0.0000\n",
      "\n",
      "Selected final action: 1\n",
      "================================================================================\n",
      "\n",
      "Step  2: new state = [0.9287589389352269, 29.941882534332425], chosen action = 1, reward w.r.t prev state = -607.33\n",
      "\n",
      "================================================================================\n",
      "STATE (t): [28.330065983723934, 20.32493798390305]\n",
      "\n",
      "Per-variable memberships:\n",
      "  Input 1: S1:0.0000, S2:1.0000\n",
      "  Input 2: S1:0.9838, S2:0.0000\n",
      "\n",
      "Normalized rule strengths (R_t):\n",
      "  Rule  1: R_t = 0.0000\n",
      "  Rule  2: R_t = 0.0000\n",
      "  Rule  3: R_t = 1.0000\n",
      "  Rule  4: R_t = 0.0000\n",
      "\n",
      "State value V_t = sum_j R_t[j] * max_a Q[j,a] = 0.0000\n",
      "Aggregated Q_prev (for previous state): 0.0000\n",
      "Received reward r_t = -680.5904\n",
      "\n",
      "TD error δ = r_t + γ * V_t - Q_prev = -680.5904\n",
      "\n",
      "Updating Q-table for rules active in previous state (showing changed entries):\n",
      "  Rule  1, action 1: Q_before=0.0000 + (α*δ*R_prev)=-68.0590 -> Q_after=-68.0590\n",
      "\n",
      "Per-rule action choices for current state (ε-greedy per rule):\n",
      "  Rule  1: chosen action = 2 (exploit)\n",
      "  Rule  2: chosen action = 0 (exploit)\n",
      "  Rule  3: chosen action = 1 (exploit)\n",
      "  Rule  4: chosen action = 1 (explore)\n",
      "\n",
      "Aggregate action weights (W[a] = sum_j R_t[j] * Q[j,a]):\n",
      "  Action 0: W = -2.2200\n",
      "  Action 1: W = 0.0000\n",
      "  Action 2: W = 0.0000\n",
      "\n",
      "Selected final action: 1\n",
      "================================================================================\n",
      "\n",
      "Step  3: new state = [28.330065983723934, 20.32493798390305], chosen action = 1, reward w.r.t prev state = -680.59\n",
      "\n",
      "================================================================================\n",
      "STATE (t): [3.577359678194705, 38.99636503186687]\n",
      "\n",
      "Per-variable memberships:\n",
      "  Input 1: S1:1.0000, S2:0.0000\n",
      "  Input 2: S1:0.0502, S2:0.4498\n",
      "\n",
      "Normalized rule strengths (R_t):\n",
      "  Rule  1: R_t = 0.1004\n",
      "  Rule  2: R_t = 0.8996\n",
      "  Rule  3: R_t = 0.0000\n",
      "  Rule  4: R_t = 0.0000\n",
      "\n",
      "State value V_t = sum_j R_t[j] * max_a Q[j,a] = 0.0000\n",
      "Aggregated Q_prev (for previous state): 0.0000\n",
      "Received reward r_t = -398.1974\n",
      "\n",
      "TD error δ = r_t + γ * V_t - Q_prev = -398.1974\n",
      "\n",
      "Updating Q-table for rules active in previous state (showing changed entries):\n",
      "  Rule  3, action 1: Q_before=0.0000 + (α*δ*R_prev)=-39.8197 -> Q_after=-39.8197\n",
      "\n",
      "Per-rule action choices for current state (ε-greedy per rule):\n",
      "  Rule  1: chosen action = 2 (exploit)\n",
      "  Rule  2: chosen action = 0 (exploit)\n",
      "  Rule  3: chosen action = 2 (exploit)\n",
      "  Rule  4: chosen action = 1 (explore)\n",
      "\n",
      "Aggregate action weights (W[a] = sum_j R_t[j] * Q[j,a]):\n",
      "  Action 0: W = -0.1485\n",
      "  Action 1: W = -8.1621\n",
      "  Action 2: W = 0.0000\n",
      "\n",
      "Selected final action: 2\n",
      "================================================================================\n",
      "\n",
      "Step  4: new state = [3.577359678194705, 38.99636503186687], chosen action = 2, reward w.r.t prev state = -398.20\n",
      "\n",
      "================================================================================\n",
      "STATE (t): [18.767983200914525, 68.65578819896854]\n",
      "\n",
      "Per-variable memberships:\n",
      "  Input 1: S1:0.1232, S2:0.3768\n",
      "  Input 2: S1:0.0000, S2:1.0000\n",
      "\n",
      "Normalized rule strengths (R_t):\n",
      "  Rule  1: R_t = 0.0000\n",
      "  Rule  2: R_t = 0.2464\n",
      "  Rule  3: R_t = 0.0000\n",
      "  Rule  4: R_t = 0.7536\n",
      "\n",
      "State value V_t = sum_j R_t[j] * max_a Q[j,a] = 0.0000\n",
      "Aggregated Q_prev (for previous state): 0.0000\n",
      "Received reward r_t = -459.9368\n",
      "\n",
      "TD error δ = r_t + γ * V_t - Q_prev = -459.9368\n",
      "\n",
      "Updating Q-table for rules active in previous state (showing changed entries):\n",
      "  Rule  1, action 2: Q_before=0.0000 + (α*δ*R_prev)=-4.6161 -> Q_after=-4.6161\n",
      "  Rule  2, action 0: Q_before=0.0000 + (α*δ*R_prev)=-41.3776 -> Q_after=-41.3776\n",
      "\n",
      "Per-rule action choices for current state (ε-greedy per rule):\n",
      "  Rule  1: chosen action = 0 (exploit)\n",
      "  Rule  2: chosen action = 2 (exploit)\n",
      "  Rule  3: chosen action = 2 (exploit)\n",
      "  Rule  4: chosen action = 2 (exploit)\n",
      "\n",
      "Aggregate action weights (W[a] = sum_j R_t[j] * Q[j,a]):\n",
      "  Action 0: W = -11.8686\n",
      "  Action 1: W = -46.1330\n",
      "  Action 2: W = 0.0000\n",
      "\n",
      "Selected final action: 2\n",
      "================================================================================\n",
      "\n",
      "Step  5: new state = [18.767983200914525, 68.65578819896854], chosen action = 2, reward w.r.t prev state = -459.94\n",
      "\n",
      "================================================================================\n",
      "STATE (t): [30.159741510877204, 48.867607262838106]\n",
      "\n",
      "Per-variable memberships:\n",
      "  Input 1: S1:0.0000, S2:1.0000\n",
      "  Input 2: S1:0.0000, S2:0.9434\n",
      "\n",
      "Normalized rule strengths (R_t):\n",
      "  Rule  1: R_t = 0.0000\n",
      "  Rule  2: R_t = 0.0000\n",
      "  Rule  3: R_t = 0.0000\n",
      "  Rule  4: R_t = 1.0000\n",
      "\n",
      "State value V_t = sum_j R_t[j] * max_a Q[j,a] = 0.0000\n",
      "Aggregated Q_prev (for previous state): 0.0000\n",
      "Received reward r_t = -859.9922\n",
      "\n",
      "TD error δ = r_t + γ * V_t - Q_prev = -859.9922\n",
      "\n",
      "Updating Q-table for rules active in previous state (showing changed entries):\n",
      "  Rule  2, action 2: Q_before=0.0000 + (α*δ*R_prev)=-21.1905 -> Q_after=-21.1905\n",
      "  Rule  4, action 2: Q_before=0.0000 + (α*δ*R_prev)=-64.8087 -> Q_after=-64.8087\n",
      "\n",
      "Per-rule action choices for current state (ε-greedy per rule):\n",
      "  Rule  1: chosen action = 0 (exploit)\n",
      "  Rule  2: chosen action = 0 (explore)\n",
      "  Rule  3: chosen action = 2 (exploit)\n",
      "  Rule  4: chosen action = 0 (exploit)\n",
      "\n",
      "Aggregate action weights (W[a] = sum_j R_t[j] * Q[j,a]):\n",
      "  Action 0: W = -2.2200\n",
      "  Action 1: W = -60.7332\n",
      "  Action 2: W = -64.8087\n",
      "\n",
      "Selected final action: 0\n",
      "================================================================================\n",
      "\n",
      "Step  6: new state = [30.159741510877204, 48.867607262838106], chosen action = 0, reward w.r.t prev state = -859.99\n",
      "\n",
      "================================================================================\n",
      "STATE (t): [29.936120235530144, 63.324183337763486]\n",
      "\n",
      "Per-variable memberships:\n",
      "  Input 1: S1:0.0000, S2:1.0000\n",
      "  Input 2: S1:0.0000, S2:1.0000\n",
      "\n",
      "Normalized rule strengths (R_t):\n",
      "  Rule  1: R_t = 0.0000\n",
      "  Rule  2: R_t = 0.0000\n",
      "  Rule  3: R_t = 0.0000\n",
      "  Rule  4: R_t = 1.0000\n",
      "\n",
      "State value V_t = sum_j R_t[j] * max_a Q[j,a] = -2.2200\n",
      "Aggregated Q_prev (for previous state): -2.2200\n",
      "Received reward r_t = -105.2574\n",
      "\n",
      "TD error δ = r_t + γ * V_t - Q_prev = -105.0354\n",
      "\n",
      "Updating Q-table for rules active in previous state (showing changed entries):\n",
      "  Rule  4, action 0: Q_before=-2.2200 + (α*δ*R_prev)=-10.5035 -> Q_after=-12.7235\n",
      "\n",
      "Per-rule action choices for current state (ε-greedy per rule):\n",
      "  Rule  1: chosen action = 0 (exploit)\n",
      "  Rule  2: chosen action = 1 (exploit)\n",
      "  Rule  3: chosen action = 2 (exploit)\n",
      "  Rule  4: chosen action = 1 (explore)\n",
      "\n",
      "Aggregate action weights (W[a] = sum_j R_t[j] * Q[j,a]):\n",
      "  Action 0: W = -12.7235\n",
      "  Action 1: W = -60.7332\n",
      "  Action 2: W = -64.8087\n",
      "\n",
      "Selected final action: 0\n",
      "================================================================================\n",
      "\n",
      "Step  7: new state = [29.936120235530144, 63.324183337763486], chosen action = 0, reward w.r.t prev state = -105.26\n",
      "\n",
      "================================================================================\n",
      "STATE (t): [7.332746077002069, 33.34889110245567]\n",
      "\n",
      "Per-variable memberships:\n",
      "  Input 1: S1:1.0000, S2:0.0000\n",
      "  Input 2: S1:0.3326, S2:0.1674\n",
      "\n",
      "Normalized rule strengths (R_t):\n",
      "  Rule  1: R_t = 0.6651\n",
      "  Rule  2: R_t = 0.3349\n",
      "  Rule  3: R_t = 0.0000\n",
      "  Rule  4: R_t = 0.0000\n",
      "\n",
      "State value V_t = sum_j R_t[j] * max_a Q[j,a] = -1.4800\n",
      "Aggregated Q_prev (for previous state): -60.7332\n",
      "Received reward r_t = -568.3828\n",
      "\n",
      "TD error δ = r_t + γ * V_t - Q_prev = -508.9816\n",
      "\n",
      "Updating Q-table for rules active in previous state (showing changed entries):\n",
      "  Rule  4, action 1: Q_before=-60.7332 + (α*δ*R_prev)=-50.8982 -> Q_after=-111.6314\n",
      "\n",
      "Per-rule action choices for current state (ε-greedy per rule):\n",
      "  Rule  1: chosen action = 0 (exploit)\n",
      "  Rule  2: chosen action = 1 (exploit)\n",
      "  Rule  3: chosen action = 2 (exploit)\n",
      "  Rule  4: chosen action = 2 (explore)\n",
      "\n",
      "Aggregate action weights (W[a] = sum_j R_t[j] * Q[j,a]):\n",
      "  Action 0: W = -14.8413\n",
      "  Action 1: W = -45.7624\n",
      "  Action 2: W = -10.1667\n",
      "\n",
      "Selected final action: 2\n",
      "================================================================================\n",
      "\n",
      "Step  8: new state = [7.332746077002069, 33.34889110245567], chosen action = 2, reward w.r.t prev state = -568.38\n",
      "\n",
      "================================================================================\n",
      "STATE (t): [8.568382380299918, 43.11301059255579]\n",
      "\n",
      "Per-variable memberships:\n",
      "  Input 1: S1:1.0000, S2:0.0000\n",
      "  Input 2: S1:0.0000, S2:0.6557\n",
      "\n",
      "Normalized rule strengths (R_t):\n",
      "  Rule  1: R_t = 0.0000\n",
      "  Rule  2: R_t = 1.0000\n",
      "  Rule  3: R_t = 0.0000\n",
      "  Rule  4: R_t = 0.0000\n",
      "\n",
      "State value V_t = sum_j R_t[j] * max_a Q[j,a] = -1.4800\n",
      "Aggregated Q_prev (for previous state): -1.4800\n",
      "Received reward r_t = -356.3691\n",
      "\n",
      "TD error δ = r_t + γ * V_t - Q_prev = -356.2211\n",
      "\n",
      "Updating Q-table for rules active in previous state (showing changed entries):\n",
      "  Rule  1, action 0: Q_before=-1.4800 + (α*δ*R_prev)=-23.6927 -> Q_after=-25.1727\n",
      "  Rule  2, action 1: Q_before=-1.4800 + (α*δ*R_prev)=-11.9295 -> Q_after=-13.4095\n",
      "\n",
      "Per-rule action choices for current state (ε-greedy per rule):\n",
      "  Rule  1: chosen action = 2 (exploit)\n",
      "  Rule  2: chosen action = 1 (exploit)\n",
      "  Rule  3: chosen action = 2 (exploit)\n",
      "  Rule  4: chosen action = 0 (exploit)\n",
      "\n",
      "Aggregate action weights (W[a] = sum_j R_t[j] * Q[j,a]):\n",
      "  Action 0: W = -41.3776\n",
      "  Action 1: W = -13.4095\n",
      "  Action 2: W = -21.1905\n",
      "\n",
      "Selected final action: 1\n",
      "================================================================================\n",
      "\n",
      "Step  9: new state = [8.568382380299918, 43.11301059255579], chosen action = 1, reward w.r.t prev state = -356.37\n",
      "\n",
      "================================================================================\n",
      "STATE (t): [11.349895704536264, 58.41567887961861]\n",
      "\n",
      "Per-variable memberships:\n",
      "  Input 1: S1:0.8650, S2:0.0000\n",
      "  Input 2: S1:0.0000, S2:1.0000\n",
      "\n",
      "Normalized rule strengths (R_t):\n",
      "  Rule  1: R_t = 0.0000\n",
      "  Rule  2: R_t = 1.0000\n",
      "  Rule  3: R_t = 0.0000\n",
      "  Rule  4: R_t = 0.0000\n",
      "\n",
      "State value V_t = sum_j R_t[j] * max_a Q[j,a] = -13.4095\n",
      "Aggregated Q_prev (for previous state): -13.4095\n",
      "Received reward r_t = -279.6889\n",
      "\n",
      "TD error δ = r_t + γ * V_t - Q_prev = -278.3479\n",
      "\n",
      "Updating Q-table for rules active in previous state (showing changed entries):\n",
      "  Rule  2, action 1: Q_before=-13.4095 + (α*δ*R_prev)=-27.8348 -> Q_after=-41.2443\n",
      "\n",
      "Per-rule action choices for current state (ε-greedy per rule):\n",
      "  Rule  1: chosen action = 0 (explore)\n",
      "  Rule  2: chosen action = 2 (exploit)\n",
      "  Rule  3: chosen action = 2 (exploit)\n",
      "  Rule  4: chosen action = 2 (explore)\n",
      "\n",
      "Aggregate action weights (W[a] = sum_j R_t[j] * Q[j,a]):\n",
      "  Action 0: W = -41.3776\n",
      "  Action 1: W = -41.2443\n",
      "  Action 2: W = -21.1905\n",
      "\n",
      "Selected final action: 2\n",
      "================================================================================\n",
      "\n",
      "Step 10: new state = [11.349895704536264, 58.41567887961861], chosen action = 2, reward w.r.t prev state = -279.69\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Temperature fuzzy sets: Low, High\n",
    "temperature = InputStateVariable(\n",
    "    Trapezium(0, 0, 10, 20),   # Low\n",
    "    Trapezium(15, 25, 35, 35)  # High\n",
    ")\n",
    "\n",
    "# Speed fuzzy sets: Slow, Fast\n",
    "speed = InputStateVariable(\n",
    "    Trapezium(0, 0, 20, 40),   # Slow\n",
    "    Trapezium(30, 50, 70, 70)  # Fast\n",
    ")\n",
    "\n",
    "fis = Build(temperature, speed)\n",
    "\n",
    "# Create FQL model with 3 possible actions\n",
    "fql = FQLModelVerbose(gamma=0.9, alpha=0.1, epsilon=0.2, action_set_length=3, fis=fis)\n",
    "\n",
    "# Example environment reward\n",
    "def env_reward(state):\n",
    "    t, s = state\n",
    "    return -((t - 25) ** 2) - ((s - 40) ** 2)\n",
    "\n",
    "# Initial state and first action (no previous state to update)\n",
    "state = [18, 35]\n",
    "print(\"=== INITIAL ACTION SELECTION ===\")\n",
    "first_action = fql.step_verbose(state, reward=0.0)  # reward=0 for initialization\n",
    "print(f\"Initial chosen action: {first_action}\\n\\n\")\n",
    "\n",
    "# Run for a few steps\n",
    "for step in range(10):\n",
    "    # reward computed for the *previous* state \n",
    "    reward = env_reward(state)\n",
    "    # new random state (the agent will observe this and use it to update Q using the previous reward)\n",
    "    state = [random.uniform(0, 35), random.uniform(20, 70)]\n",
    "    action = fql.step_verbose(state, reward)\n",
    "    print(f\"Step {step+1:>2}: new state = {state}, chosen action = {action}, reward w.r.t prev state = {reward:.2f}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
