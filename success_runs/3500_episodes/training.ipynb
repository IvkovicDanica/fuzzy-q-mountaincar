{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744f41e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.trainer import train\n",
    "import random\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d2bee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 10 reward -9.0 MA(100) 0.20 eps 0.990\n",
      "Ep 20 reward -2.0 MA(100) 2.05 eps 0.980\n",
      "Ep 30 reward -6.0 MA(100) 0.90 eps 0.970\n",
      "Ep 40 reward 3.0 MA(100) 0.75 eps 0.961\n",
      "Ep 50 reward -4.0 MA(100) 0.42 eps 0.951\n",
      "Ep 60 reward -3.0 MA(100) -0.40 eps 0.942\n",
      "Ep 70 reward -5.0 MA(100) -1.03 eps 0.932\n",
      "Ep 80 reward -11.0 MA(100) -1.65 eps 0.923\n",
      "Ep 90 reward -7.0 MA(100) -2.33 eps 0.914\n",
      "Ep 100 reward -10.0 MA(100) -2.62 eps 0.905\n",
      "q_table stats: min -2.578, mean -0.124, max 1.409\n",
      "Ep 110 reward -12.0 MA(100) -3.34 eps 0.896\n",
      "Ep 120 reward -5.0 MA(100) -4.40 eps 0.887\n",
      "Ep 130 reward -6.0 MA(100) -4.76 eps 0.878\n",
      "Ep 140 reward -3.0 MA(100) -5.33 eps 0.869\n",
      "Ep 150 reward -6.0 MA(100) -5.60 eps 0.861\n",
      "Ep 160 reward 3.0 MA(100) -5.25 eps 0.852\n",
      "Ep 170 reward -4.0 MA(100) -4.79 eps 0.844\n",
      "Ep 180 reward 4.0 MA(100) -4.12 eps 0.835\n",
      "Ep 190 reward 6.0 MA(100) -3.54 eps 0.827\n",
      "Ep 200 reward -3.0 MA(100) -3.16 eps 0.819\n",
      "q_table stats: min -4.960, mean -0.375, max 2.436\n",
      "Ep 210 reward 26.0 MA(100) -2.15 eps 0.810\n",
      "Ep 220 reward 4.0 MA(100) -1.62 eps 0.802\n",
      "Ep 230 reward -5.0 MA(100) -1.05 eps 0.794\n",
      "Ep 240 reward 94.0 MA(100) 7.16 eps 0.787\n",
      "Ep 250 reward 105.0 MA(100) 16.25 eps 0.779\n",
      "Ep 260 reward 104.0 MA(100) 26.70 eps 0.771\n",
      "Ep 270 reward 95.0 MA(100) 37.21 eps 0.763\n",
      "Ep 280 reward 94.0 MA(100) 45.83 eps 0.756\n",
      "Ep 290 reward 90.0 MA(100) 53.34 eps 0.748\n",
      "Ep 300 reward 90.0 MA(100) 62.84 eps 0.741\n",
      "q_table stats: min -4.572, mean 1.727, max 10.777\n",
      "Ep 310 reward -9.0 MA(100) 69.82 eps 0.733\n",
      "Ep 320 reward 87.0 MA(100) 79.16 eps 0.726\n",
      "Ep 330 reward 90.0 MA(100) 87.16 eps 0.719\n",
      "Ep 340 reward 92.0 MA(100) 88.65 eps 0.712\n",
      "Ep 350 reward 93.0 MA(100) 89.51 eps 0.705\n",
      "Ep 360 reward 101.0 MA(100) 87.96 eps 0.698\n",
      "Ep 370 reward 100.0 MA(100) 83.03 eps 0.691\n",
      "Ep 380 reward 111.0 MA(100) 82.31 eps 0.684\n",
      "Ep 390 reward 107.0 MA(100) 82.37 eps 0.677\n",
      "Ep 400 reward 103.0 MA(100) 83.87 eps 0.670\n",
      "q_table stats: min -2.959, mean 4.938, max 22.987\n",
      "Ep 410 reward 102.0 MA(100) 82.37 eps 0.664\n",
      "Ep 420 reward 106.0 MA(100) 83.02 eps 0.657\n",
      "Ep 430 reward 100.0 MA(100) 80.39 eps 0.650\n",
      "Ep 440 reward 97.0 MA(100) 80.28 eps 0.644\n",
      "Ep 450 reward 103.0 MA(100) 80.72 eps 0.637\n",
      "Ep 460 reward 92.0 MA(100) 80.88 eps 0.631\n",
      "Ep 470 reward 94.0 MA(100) 84.86 eps 0.625\n",
      "Ep 480 reward 97.0 MA(100) 86.52 eps 0.619\n",
      "Ep 490 reward 115.0 MA(100) 87.20 eps 0.612\n",
      "Ep 500 reward 105.0 MA(100) 87.76 eps 0.606\n",
      "q_table stats: min -1.686, mean 8.435, max 36.931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Andrija Lukic\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pygame\\pkgdata.py:27: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 510 reward 106.0 MA(100) 93.09 eps 0.600\n",
      "Ep 520 reward 105.0 MA(100) 94.02 eps 0.594\n",
      "Ep 530 reward 120.0 MA(100) 100.02 eps 0.588\n",
      "Ep 540 reward 100.0 MA(100) 101.71 eps 0.583\n",
      "Ep 550 reward 97.0 MA(100) 102.28 eps 0.577\n",
      "Ep 560 reward 116.0 MA(100) 104.16 eps 0.571\n",
      "Ep 570 reward 123.0 MA(100) 106.11 eps 0.565\n",
      "Ep 580 reward 110.0 MA(100) 108.66 eps 0.560\n",
      "Ep 590 reward 114.0 MA(100) 111.71 eps 0.554\n",
      "Ep 600 reward 115.0 MA(100) 112.30 eps 0.549\n",
      "q_table stats: min -1.686, mean 12.751, max 57.753\n",
      "Ep 610 reward 130.0 MA(100) 114.85 eps 0.543\n",
      "Ep 620 reward 136.0 MA(100) 118.15 eps 0.538\n",
      "Ep 630 reward 158.0 MA(100) 122.68 eps 0.532\n",
      "Ep 640 reward 161.0 MA(100) 130.39 eps 0.527\n",
      "Ep 650 reward 148.0 MA(100) 136.53 eps 0.522\n",
      "Ep 660 reward 182.0 MA(100) 144.08 eps 0.517\n",
      "Ep 670 reward 162.0 MA(100) 150.82 eps 0.512\n",
      "Ep 680 reward 189.0 MA(100) 156.54 eps 0.506\n",
      "Ep 690 reward 171.0 MA(100) 161.75 eps 0.501\n",
      "Ep 700 reward 190.0 MA(100) 167.98 eps 0.496\n",
      "q_table stats: min -1.686, mean 16.977, max 84.902\n",
      "Ep 710 reward 153.0 MA(100) 170.85 eps 0.491\n",
      "Ep 720 reward 181.0 MA(100) 174.33 eps 0.487\n",
      "Ep 730 reward 212.0 MA(100) 181.48 eps 0.482\n",
      "Ep 740 reward 203.0 MA(100) 183.37 eps 0.477\n",
      "Ep 750 reward 298.0 MA(100) 191.08 eps 0.472\n",
      "Ep 760 reward 480.0 MA(100) 206.11 eps 0.467\n",
      "Ep 770 reward 261.0 MA(100) 215.46 eps 0.463\n",
      "Ep 780 reward 309.0 MA(100) 231.71 eps 0.458\n",
      "Ep 790 reward 215.0 MA(100) 234.01 eps 0.454\n",
      "Ep 800 reward 480.0 MA(100) 255.69 eps 0.449\n",
      "q_table stats: min -1.688, mean 20.946, max 112.592\n",
      "Ep 810 reward 81.0 MA(100) 274.88 eps 0.445\n",
      "Ep 820 reward 480.0 MA(100) 298.46 eps 0.440\n",
      "Ep 830 reward 136.0 MA(100) 316.10 eps 0.436\n",
      "Ep 840 reward 480.0 MA(100) 332.59 eps 0.432\n",
      "Ep 850 reward 480.0 MA(100) 343.05 eps 0.427\n",
      "Ep 860 reward 105.0 MA(100) 318.97 eps 0.423\n",
      "Ep 870 reward 480.0 MA(100) 332.69 eps 0.419\n",
      "Ep 880 reward 397.0 MA(100) 345.69 eps 0.415\n",
      "Ep 890 reward 480.0 MA(100) 373.46 eps 0.410\n",
      "Ep 900 reward 109.0 MA(100) 351.02 eps 0.406\n",
      "q_table stats: min -5.248, mean 21.617, max 123.632\n",
      "Ep 910 reward 480.0 MA(100) 339.18 eps 0.402\n",
      "Ep 920 reward 480.0 MA(100) 330.46 eps 0.398\n",
      "Ep 930 reward -8.0 MA(100) 324.93 eps 0.394\n",
      "Ep 940 reward -3.0 MA(100) 289.68 eps 0.390\n",
      "Ep 950 reward 10.0 MA(100) 255.91 eps 0.387\n",
      "Ep 960 reward 458.0 MA(100) 259.57 eps 0.383\n",
      "Ep 970 reward -10.0 MA(100) 237.11 eps 0.379\n",
      "Ep 980 reward 173.0 MA(100) 213.44 eps 0.375\n",
      "Ep 990 reward 190.0 MA(100) 203.42 eps 0.371\n",
      "Ep 1000 reward 132.0 MA(100) 201.25 eps 0.368\n",
      "q_table stats: min -7.859, mean 19.830, max 123.026\n",
      "Ep 1010 reward 173.0 MA(100) 192.32 eps 0.364\n",
      "Ep 1020 reward 12.0 MA(100) 166.83 eps 0.360\n",
      "Ep 1030 reward 184.0 MA(100) 149.80 eps 0.357\n",
      "Ep 1040 reward 48.0 MA(100) 168.10 eps 0.353\n",
      "Ep 1050 reward 231.0 MA(100) 198.59 eps 0.350\n",
      "Ep 1060 reward 13.0 MA(100) 201.57 eps 0.346\n",
      "Ep 1070 reward 4.0 MA(100) 189.13 eps 0.343\n",
      "Ep 1080 reward 480.0 MA(100) 179.32 eps 0.339\n",
      "Ep 1090 reward 82.0 MA(100) 157.49 eps 0.336\n",
      "Ep 1100 reward 135.0 MA(100) 161.18 eps 0.333\n",
      "q_table stats: min -11.283, mean 18.201, max 118.175\n",
      "Ep 1110 reward 120.0 MA(100) 161.73 eps 0.329\n",
      "Ep 1120 reward 188.0 MA(100) 174.78 eps 0.326\n",
      "Ep 1130 reward 243.0 MA(100) 174.47 eps 0.323\n",
      "Ep 1140 reward 242.0 MA(100) 172.99 eps 0.320\n",
      "Ep 1150 reward 129.0 MA(100) 154.95 eps 0.316\n",
      "Ep 1160 reward 114.0 MA(100) 160.99 eps 0.313\n",
      "Ep 1170 reward 300.0 MA(100) 174.49 eps 0.310\n",
      "Ep 1180 reward 125.0 MA(100) 178.64 eps 0.307\n",
      "Ep 1190 reward 186.0 MA(100) 183.24 eps 0.304\n",
      "Ep 1200 reward 286.0 MA(100) 180.87 eps 0.301\n",
      "q_table stats: min -13.633, mean 17.439, max 116.150\n",
      "Ep 1210 reward 152.0 MA(100) 177.09 eps 0.298\n",
      "Ep 1220 reward -8.0 MA(100) 171.15 eps 0.295\n",
      "Ep 1230 reward 171.0 MA(100) 170.31 eps 0.292\n",
      "Ep 1240 reward 145.0 MA(100) 167.48 eps 0.289\n",
      "Ep 1250 reward 125.0 MA(100) 166.14 eps 0.286\n",
      "Ep 1260 reward 126.0 MA(100) 162.90 eps 0.283\n",
      "Ep 1270 reward 480.0 MA(100) 172.41 eps 0.281\n",
      "Ep 1280 reward 155.0 MA(100) 169.92 eps 0.278\n",
      "Ep 1290 reward 219.0 MA(100) 169.05 eps 0.275\n",
      "Ep 1300 reward 178.0 MA(100) 173.92 eps 0.272\n",
      "q_table stats: min -20.117, mean 18.175, max 120.035\n",
      "Ep 1310 reward 480.0 MA(100) 195.82 eps 0.270\n",
      "Ep 1320 reward 164.0 MA(100) 203.84 eps 0.267\n",
      "Ep 1330 reward 159.0 MA(100) 201.17 eps 0.264\n",
      "Ep 1340 reward 187.0 MA(100) 203.66 eps 0.262\n",
      "Ep 1350 reward 349.0 MA(100) 215.92 eps 0.259\n",
      "Ep 1360 reward 399.0 MA(100) 224.19 eps 0.256\n",
      "Ep 1370 reward 105.0 MA(100) 222.91 eps 0.254\n",
      "Ep 1380 reward 480.0 MA(100) 241.59 eps 0.251\n",
      "Ep 1390 reward 144.0 MA(100) 248.46 eps 0.249\n",
      "Ep 1400 reward 179.0 MA(100) 248.38 eps 0.246\n",
      "q_table stats: min -26.404, mean 19.301, max 127.046\n",
      "Ep 1410 reward 209.0 MA(100) 232.79 eps 0.244\n",
      "Ep 1420 reward 182.0 MA(100) 230.57 eps 0.242\n",
      "Ep 1430 reward 205.0 MA(100) 237.26 eps 0.239\n",
      "Ep 1440 reward 170.0 MA(100) 235.38 eps 0.237\n",
      "Ep 1450 reward 167.0 MA(100) 228.44 eps 0.234\n",
      "Ep 1460 reward 285.0 MA(100) 222.67 eps 0.232\n",
      "Ep 1470 reward 480.0 MA(100) 219.67 eps 0.230\n",
      "Ep 1480 reward 253.0 MA(100) 216.21 eps 0.227\n",
      "Ep 1490 reward 161.0 MA(100) 213.94 eps 0.225\n",
      "Ep 1500 reward 136.0 MA(100) 214.67 eps 0.223\n",
      "q_table stats: min -33.792, mean 20.923, max 133.713\n",
      "Ep 1510 reward 111.0 MA(100) 221.31 eps 0.221\n",
      "Ep 1520 reward 139.0 MA(100) 222.56 eps 0.219\n",
      "Ep 1530 reward 435.0 MA(100) 240.16 eps 0.216\n",
      "Ep 1540 reward 480.0 MA(100) 248.42 eps 0.214\n",
      "Ep 1550 reward 148.0 MA(100) 257.60 eps 0.212\n",
      "Ep 1560 reward 480.0 MA(100) 267.65 eps 0.210\n",
      "Ep 1570 reward 480.0 MA(100) 266.95 eps 0.208\n",
      "Ep 1580 reward 231.0 MA(100) 260.24 eps 0.206\n",
      "Ep 1590 reward 480.0 MA(100) 269.03 eps 0.204\n",
      "Ep 1600 reward 228.0 MA(100) 271.93 eps 0.202\n",
      "q_table stats: min -37.650, mean 21.169, max 135.551\n",
      "Ep 1610 reward 297.0 MA(100) 273.75 eps 0.200\n",
      "Ep 1620 reward 137.0 MA(100) 278.29 eps 0.198\n",
      "Ep 1630 reward 248.0 MA(100) 267.25 eps 0.196\n",
      "Ep 1640 reward 480.0 MA(100) 267.82 eps 0.194\n",
      "Ep 1650 reward 480.0 MA(100) 278.57 eps 0.192\n",
      "Ep 1660 reward 120.0 MA(100) 276.64 eps 0.190\n",
      "Ep 1670 reward 167.0 MA(100) 274.74 eps 0.188\n",
      "Ep 1680 reward 286.0 MA(100) 278.49 eps 0.186\n",
      "Ep 1690 reward 116.0 MA(100) 276.97 eps 0.184\n",
      "Ep 1700 reward 480.0 MA(100) 286.65 eps 0.183\n",
      "q_table stats: min -42.282, mean 21.782, max 136.444\n",
      "Ep 1710 reward 480.0 MA(100) 293.70 eps 0.181\n",
      "Ep 1720 reward 268.0 MA(100) 300.10 eps 0.179\n",
      "Ep 1730 reward 192.0 MA(100) 300.69 eps 0.177\n",
      "Ep 1740 reward 196.0 MA(100) 304.01 eps 0.175\n",
      "Ep 1750 reward 96.0 MA(100) 294.39 eps 0.174\n",
      "Ep 1760 reward 275.0 MA(100) 291.44 eps 0.172\n",
      "Ep 1770 reward 480.0 MA(100) 300.49 eps 0.170\n",
      "Ep 1780 reward 236.0 MA(100) 295.27 eps 0.168\n",
      "Ep 1790 reward 121.0 MA(100) 292.61 eps 0.167\n",
      "Ep 1800 reward 135.0 MA(100) 292.27 eps 0.165\n",
      "q_table stats: min -46.651, mean 22.264, max 133.920\n",
      "Ep 1810 reward 105.0 MA(100) 287.03 eps 0.164\n",
      "Ep 1820 reward 122.0 MA(100) 285.61 eps 0.162\n",
      "Ep 1830 reward 275.0 MA(100) 286.06 eps 0.160\n",
      "Ep 1840 reward 480.0 MA(100) 289.32 eps 0.159\n",
      "Ep 1850 reward 480.0 MA(100) 290.74 eps 0.157\n",
      "Ep 1860 reward 331.0 MA(100) 293.21 eps 0.156\n",
      "Ep 1870 reward 480.0 MA(100) 293.93 eps 0.154\n",
      "Ep 1880 reward 480.0 MA(100) 299.38 eps 0.152\n",
      "Ep 1890 reward 480.0 MA(100) 302.92 eps 0.151\n",
      "Ep 1900 reward 165.0 MA(100) 294.64 eps 0.149\n",
      "q_table stats: min -48.611, mean 22.801, max 131.563\n",
      "Ep 1910 reward 134.0 MA(100) 297.33 eps 0.148\n",
      "Ep 1920 reward 480.0 MA(100) 296.67 eps 0.146\n",
      "Ep 1930 reward 329.0 MA(100) 293.72 eps 0.145\n",
      "Ep 1940 reward 136.0 MA(100) 298.15 eps 0.144\n",
      "Ep 1950 reward 123.0 MA(100) 292.38 eps 0.142\n",
      "Ep 1960 reward 329.0 MA(100) 289.99 eps 0.141\n",
      "Ep 1970 reward 345.0 MA(100) 291.93 eps 0.139\n",
      "Ep 1980 reward 480.0 MA(100) 309.21 eps 0.138\n",
      "Ep 1990 reward 171.0 MA(100) 306.78 eps 0.137\n",
      "Ep 2000 reward 480.0 MA(100) 311.77 eps 0.135\n",
      "q_table stats: min -48.961, mean 22.989, max 127.939\n",
      "Ep 2010 reward 351.0 MA(100) 314.88 eps 0.134\n",
      "Ep 2020 reward 480.0 MA(100) 322.17 eps 0.133\n",
      "Ep 2030 reward 161.0 MA(100) 327.71 eps 0.131\n",
      "Ep 2040 reward 480.0 MA(100) 326.74 eps 0.130\n",
      "Ep 2050 reward 480.0 MA(100) 340.35 eps 0.129\n",
      "Ep 2060 reward 480.0 MA(100) 353.50 eps 0.127\n",
      "Ep 2070 reward 111.0 MA(100) 353.98 eps 0.126\n",
      "Ep 2080 reward 135.0 MA(100) 343.35 eps 0.125\n",
      "Ep 2090 reward 366.0 MA(100) 355.25 eps 0.124\n",
      "Ep 2100 reward 246.0 MA(100) 355.53 eps 0.122\n",
      "q_table stats: min -48.242, mean 23.852, max 138.310\n",
      "Ep 2110 reward 287.0 MA(100) 350.83 eps 0.121\n",
      "Ep 2120 reward 303.0 MA(100) 341.29 eps 0.120\n",
      "Ep 2130 reward 480.0 MA(100) 349.40 eps 0.119\n",
      "Ep 2140 reward 480.0 MA(100) 357.32 eps 0.118\n",
      "Ep 2150 reward 480.0 MA(100) 364.67 eps 0.116\n",
      "Ep 2160 reward 480.0 MA(100) 372.36 eps 0.115\n",
      "Ep 2170 reward 480.0 MA(100) 379.80 eps 0.114\n",
      "Ep 2180 reward 131.0 MA(100) 387.23 eps 0.113\n",
      "Ep 2190 reward 166.0 MA(100) 380.95 eps 0.112\n",
      "Ep 2200 reward 480.0 MA(100) 385.63 eps 0.111\n",
      "q_table stats: min -47.120, mean 25.174, max 158.302\n",
      "Ep 2210 reward 222.0 MA(100) 395.95 eps 0.110\n",
      "Ep 2220 reward 480.0 MA(100) 406.41 eps 0.108\n",
      "Ep 2230 reward 182.0 MA(100) 403.06 eps 0.107\n",
      "Ep 2240 reward 394.0 MA(100) 386.85 eps 0.106\n",
      "Ep 2250 reward 480.0 MA(100) 378.32 eps 0.105\n",
      "Ep 2260 reward 480.0 MA(100) 378.71 eps 0.104\n",
      "Ep 2270 reward 421.0 MA(100) 383.94 eps 0.103\n",
      "Ep 2280 reward 480.0 MA(100) 388.85 eps 0.102\n",
      "Ep 2290 reward 142.0 MA(100) 392.21 eps 0.101\n",
      "Ep 2300 reward 153.0 MA(100) 382.76 eps 0.100\n",
      "q_table stats: min -46.163, mean 24.974, max 161.940\n",
      "Ep 2310 reward 154.0 MA(100) 364.05 eps 0.099\n",
      "Ep 2320 reward 147.0 MA(100) 346.54 eps 0.098\n",
      "Ep 2330 reward 139.0 MA(100) 331.64 eps 0.097\n",
      "Ep 2340 reward 480.0 MA(100) 346.11 eps 0.096\n",
      "Ep 2350 reward 184.0 MA(100) 329.20 eps 0.095\n",
      "Ep 2360 reward 148.0 MA(100) 297.82 eps 0.094\n",
      "Ep 2370 reward 250.0 MA(100) 272.02 eps 0.093\n",
      "Ep 2380 reward 246.0 MA(100) 250.61 eps 0.092\n",
      "Ep 2390 reward 200.0 MA(100) 233.22 eps 0.092\n",
      "Ep 2400 reward 371.0 MA(100) 226.81 eps 0.091\n",
      "q_table stats: min -46.164, mean 22.583, max 159.484\n",
      "Ep 2410 reward 295.0 MA(100) 227.33 eps 0.090\n",
      "Ep 2420 reward 237.0 MA(100) 225.49 eps 0.089\n",
      "Ep 2430 reward 263.0 MA(100) 222.41 eps 0.088\n",
      "Ep 2440 reward 414.0 MA(100) 209.91 eps 0.087\n",
      "Ep 2450 reward 480.0 MA(100) 238.86 eps 0.086\n",
      "Ep 2460 reward 480.0 MA(100) 270.48 eps 0.085\n",
      "Ep 2470 reward 480.0 MA(100) 296.63 eps 0.084\n",
      "Ep 2480 reward 480.0 MA(100) 319.23 eps 0.084\n",
      "Ep 2490 reward 300.0 MA(100) 345.26 eps 0.083\n",
      "Ep 2500 reward 71.0 MA(100) 351.28 eps 0.082\n",
      "q_table stats: min -44.786, mean 23.287, max 148.904\n",
      "Ep 2510 reward 35.0 MA(100) 332.00 eps 0.081\n",
      "Ep 2520 reward 111.0 MA(100) 324.78 eps 0.080\n",
      "Ep 2530 reward 126.0 MA(100) 319.48 eps 0.080\n",
      "Ep 2540 reward 141.0 MA(100) 302.71 eps 0.079\n",
      "Ep 2550 reward 107.0 MA(100) 266.75 eps 0.078\n",
      "Ep 2560 reward 127.0 MA(100) 232.73 eps 0.077\n",
      "Ep 2570 reward 165.0 MA(100) 197.02 eps 0.076\n",
      "Ep 2580 reward 119.0 MA(100) 161.56 eps 0.076\n",
      "Ep 2590 reward 153.0 MA(100) 128.80 eps 0.075\n",
      "Ep 2600 reward 159.0 MA(100) 115.18 eps 0.074\n",
      "q_table stats: min -44.859, mean 19.308, max 146.554\n",
      "Ep 2610 reward 119.0 MA(100) 126.21 eps 0.073\n",
      "Ep 2620 reward 95.0 MA(100) 128.31 eps 0.073\n",
      "Ep 2630 reward 132.0 MA(100) 129.04 eps 0.072\n",
      "Ep 2640 reward 125.0 MA(100) 129.68 eps 0.071\n",
      "Ep 2650 reward 184.0 MA(100) 130.27 eps 0.071\n",
      "Ep 2660 reward 100.0 MA(100) 130.20 eps 0.070\n",
      "Ep 2670 reward 115.0 MA(100) 130.09 eps 0.069\n",
      "Ep 2680 reward 116.0 MA(100) 128.77 eps 0.068\n",
      "Ep 2690 reward 121.0 MA(100) 127.26 eps 0.068\n",
      "Ep 2700 reward 134.0 MA(100) 129.23 eps 0.067\n",
      "q_table stats: min -44.900, mean 16.565, max 144.018\n",
      "Ep 2710 reward 289.0 MA(100) 132.48 eps 0.066\n",
      "Ep 2720 reward 160.0 MA(100) 130.72 eps 0.066\n",
      "Ep 2730 reward 113.0 MA(100) 131.91 eps 0.065\n",
      "Ep 2740 reward 89.0 MA(100) 132.55 eps 0.064\n",
      "Ep 2750 reward 97.0 MA(100) 133.82 eps 0.064\n",
      "Ep 2760 reward 185.0 MA(100) 133.91 eps 0.063\n",
      "Ep 2770 reward 148.0 MA(100) 133.93 eps 0.063\n",
      "Ep 2780 reward 122.0 MA(100) 134.64 eps 0.062\n",
      "Ep 2790 reward 110.0 MA(100) 135.77 eps 0.061\n",
      "Ep 2800 reward 125.0 MA(100) 132.67 eps 0.061\n",
      "q_table stats: min -44.911, mean 14.468, max 141.108\n",
      "Ep 2810 reward 321.0 MA(100) 138.02 eps 0.060\n",
      "Ep 2820 reward 480.0 MA(100) 162.75 eps 0.060\n",
      "Ep 2830 reward 310.0 MA(100) 192.11 eps 0.059\n",
      "Ep 2840 reward 250.0 MA(100) 204.35 eps 0.058\n",
      "Ep 2850 reward 244.0 MA(100) 214.51 eps 0.058\n",
      "Ep 2860 reward 217.0 MA(100) 221.77 eps 0.057\n",
      "Ep 2870 reward 220.0 MA(100) 230.64 eps 0.057\n",
      "Ep 2880 reward 229.0 MA(100) 240.68 eps 0.056\n",
      "Ep 2890 reward 240.0 MA(100) 255.32 eps 0.055\n",
      "Ep 2900 reward 268.0 MA(100) 272.79 eps 0.055\n",
      "q_table stats: min -43.610, mean 17.920, max 138.189\n",
      "Ep 2910 reward 196.0 MA(100) 274.97 eps 0.054\n",
      "Ep 2920 reward 252.0 MA(100) 260.38 eps 0.054\n",
      "Ep 2930 reward 209.0 MA(100) 238.22 eps 0.053\n",
      "Ep 2940 reward 199.0 MA(100) 235.30 eps 0.053\n",
      "Ep 2950 reward 421.0 MA(100) 235.66 eps 0.052\n",
      "Ep 2960 reward 234.0 MA(100) 242.67 eps 0.052\n",
      "Ep 2970 reward 480.0 MA(100) 252.28 eps 0.051\n",
      "Ep 2980 reward 374.0 MA(100) 260.01 eps 0.051\n",
      "Ep 2990 reward 480.0 MA(100) 277.71 eps 0.050\n",
      "Ep 3000 reward 196.0 MA(100) 282.58 eps 0.050\n",
      "q_table stats: min -43.016, mean 19.104, max 146.729\n",
      "Ep 3010 reward 281.0 MA(100) 287.06 eps 0.049\n",
      "Ep 3020 reward 226.0 MA(100) 292.67 eps 0.049\n",
      "Ep 3030 reward 322.0 MA(100) 296.74 eps 0.048\n",
      "Ep 3040 reward 303.0 MA(100) 300.53 eps 0.048\n",
      "Ep 3050 reward 353.0 MA(100) 308.80 eps 0.047\n",
      "Ep 3060 reward 268.0 MA(100) 307.85 eps 0.047\n",
      "Ep 3070 reward 317.0 MA(100) 304.21 eps 0.046\n",
      "Ep 3080 reward 369.0 MA(100) 310.94 eps 0.046\n",
      "Ep 3090 reward 309.0 MA(100) 309.76 eps 0.045\n",
      "Ep 3100 reward 305.0 MA(100) 312.00 eps 0.045\n",
      "q_table stats: min -42.514, mean 17.909, max 139.145\n",
      "Ep 3110 reward 332.0 MA(100) 310.96 eps 0.045\n",
      "Ep 3120 reward 219.0 MA(100) 311.80 eps 0.044\n",
      "Ep 3130 reward 310.0 MA(100) 314.09 eps 0.044\n",
      "Ep 3140 reward 271.0 MA(100) 314.83 eps 0.043\n",
      "Ep 3150 reward 244.0 MA(100) 310.57 eps 0.043\n",
      "Ep 3160 reward 272.0 MA(100) 314.44 eps 0.042\n",
      "Ep 3170 reward 295.0 MA(100) 316.95 eps 0.042\n",
      "Ep 3180 reward 274.0 MA(100) 309.24 eps 0.042\n",
      "Ep 3190 reward 236.0 MA(100) 295.92 eps 0.041\n",
      "Ep 3200 reward 250.0 MA(100) 292.74 eps 0.041\n",
      "q_table stats: min -42.065, mean 17.362, max 129.324\n",
      "Ep 3210 reward 261.0 MA(100) 293.75 eps 0.040\n",
      "Ep 3220 reward 275.0 MA(100) 294.96 eps 0.040\n",
      "Ep 3230 reward 336.0 MA(100) 293.25 eps 0.039\n",
      "Ep 3240 reward 289.0 MA(100) 300.03 eps 0.039\n",
      "Ep 3250 reward 440.0 MA(100) 307.49 eps 0.039\n",
      "Ep 3260 reward 365.0 MA(100) 308.54 eps 0.038\n",
      "Ep 3270 reward 311.0 MA(100) 312.50 eps 0.038\n",
      "Ep 3280 reward 315.0 MA(100) 314.42 eps 0.038\n",
      "Ep 3290 reward 289.0 MA(100) 315.55 eps 0.037\n",
      "Ep 3300 reward 283.0 MA(100) 313.59 eps 0.037\n",
      "q_table stats: min -43.137, mean 17.623, max 119.600\n",
      "Ep 3310 reward 396.0 MA(100) 320.83 eps 0.036\n",
      "Ep 3320 reward 480.0 MA(100) 335.76 eps 0.036\n",
      "Ep 3330 reward 285.0 MA(100) 348.51 eps 0.036\n",
      "Ep 3340 reward 304.0 MA(100) 344.78 eps 0.035\n",
      "Ep 3350 reward 343.0 MA(100) 341.14 eps 0.035\n",
      "Ep 3360 reward 326.0 MA(100) 337.99 eps 0.035\n",
      "Ep 3370 reward 281.0 MA(100) 338.14 eps 0.034\n",
      "Ep 3380 reward 480.0 MA(100) 341.70 eps 0.034\n",
      "Ep 3390 reward 365.0 MA(100) 344.81 eps 0.034\n",
      "Ep 3400 reward 480.0 MA(100) 357.65 eps 0.033\n",
      "q_table stats: min -45.928, mean 18.754, max 113.853\n",
      "Ep 3410 reward 480.0 MA(100) 369.89 eps 0.033\n",
      "Ep 3420 reward 480.0 MA(100) 373.23 eps 0.033\n",
      "Ep 3430 reward 480.0 MA(100) 381.91 eps 0.032\n",
      "Ep 3440 reward 480.0 MA(100) 399.77 eps 0.032\n",
      "Ep 3450 reward 480.0 MA(100) 415.52 eps 0.032\n",
      "Ep 3460 reward 480.0 MA(100) 434.43 eps 0.031\n",
      "Ep 3470 reward 480.0 MA(100) 449.03 eps 0.031\n",
      "Ep 3480 reward 480.0 MA(100) 462.83 eps 0.031\n",
      "Ep 3490 reward 480.0 MA(100) 475.69 eps 0.030\n",
      "Solved at episode 3490 (moving avg 475.69)\n"
     ]
    }
   ],
   "source": [
    "agent = train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c15218",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(fql_agent, num_episodes=100, render=False, seed=None, max_steps=None, verbose=True):\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    env = gym.make(config.ENV_NAME)\n",
    "    max_steps = max_steps or getattr(config, \"MAX_STEPS_PER_EPISODE\", 1000)\n",
    "\n",
    "    prev_eps = getattr(fql_agent, \"epsilon\", None)\n",
    "    if prev_eps is not None:\n",
    "        fql_agent.epsilon = 0.0\n",
    "\n",
    "    episode_rewards = []\n",
    "    try:\n",
    "        for ep in range(1, num_episodes + 1):\n",
    "            if seed is not None:\n",
    "                obs, _ = env.reset(seed=seed + ep)\n",
    "            else:\n",
    "                obs, _ = env.reset()\n",
    "\n",
    "            total_reward = 0.0\n",
    "            terminated = truncated = False\n",
    "            steps = 0\n",
    "            cart_pos, cart_vel, pole_angle, pole_ang_vel = obs\n",
    "            action = fql_agent.get_initial_action([float(pole_angle), float(pole_ang_vel)])\n",
    "\n",
    "            while not (terminated or truncated) and steps < max_steps:\n",
    "                obs, reward, terminated, truncated, _ = env.step(int(action))\n",
    "                total_reward += reward\n",
    "                steps += 1\n",
    "\n",
    "                if terminated or truncated:\n",
    "                    break\n",
    "\n",
    "                _, _, pole_angle, pole_ang_vel = obs\n",
    "                action = fql_agent.get_action([float(pole_angle), float(pole_ang_vel)])\n",
    "\n",
    "            episode_rewards.append(total_reward)\n",
    "\n",
    "            if verbose and (ep == 1 or ep % 10 == 0 or ep == num_episodes):\n",
    "                recent_ma = float(np.mean(episode_rewards[-100:]))\n",
    "                print(f\"[Test] Ep {ep}/{num_episodes} reward {total_reward:.1f} recent_MA({min(100,len(episode_rewards))}) {recent_ma:.2f}\")\n",
    "\n",
    "    finally:\n",
    "        if prev_eps is not None:\n",
    "            fql_agent.epsilon = prev_eps\n",
    "        env.close()\n",
    "\n",
    "    stats = {\n",
    "        \"mean\": float(np.mean(episode_rewards)),\n",
    "        \"std\": float(np.std(episode_rewards)),\n",
    "        \"min\": float(np.min(episode_rewards)),\n",
    "        \"max\": float(np.max(episode_rewards)),\n",
    "        \"median\": float(np.median(episode_rewards)),\n",
    "    }\n",
    "\n",
    "    if verbose:\n",
    "        print(\"=== Evaluation summary ===\")\n",
    "        print(f\"episodes: {len(episode_rewards)}  mean: {stats['mean']:.2f}  std: {stats['std']:.2f}  min: {stats['min']:.1f}  max: {stats['max']:.1f}\")\n",
    "\n",
    "    return episode_rewards, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "567d7ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] Ep 1/50 reward 500.0 recent_MA(1) 500.00\n",
      "[Test] Ep 10/50 reward 500.0 recent_MA(10) 500.00\n",
      "[Test] Ep 20/50 reward 500.0 recent_MA(20) 500.00\n",
      "[Test] Ep 30/50 reward 500.0 recent_MA(30) 500.00\n",
      "[Test] Ep 40/50 reward 500.0 recent_MA(40) 500.00\n",
      "[Test] Ep 50/50 reward 500.0 recent_MA(50) 500.00\n",
      "=== Evaluation summary ===\n",
      "episodes: 50  mean: 500.00  std: 0.00  min: 500.0  max: 500.0\n"
     ]
    }
   ],
   "source": [
    "test_rewards, test_stats = test(agent[0], num_episodes=50, render=True, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b9bb905",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(f\"q_table_best_ep{3500}.npy\", agent[0].q_table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
