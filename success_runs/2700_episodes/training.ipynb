{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33624d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.trainer import train\n",
    "import random\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d2bee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 10 reward -5.0 MA(100) 5.80 eps 0.951\n",
      "Ep 20 reward -9.0 MA(100) 1.15 eps 0.905\n",
      "Ep 30 reward -10.0 MA(100) -0.03 eps 0.860\n",
      "Ep 40 reward -11.0 MA(100) -1.68 eps 0.818\n",
      "Ep 50 reward 1.0 MA(100) -2.54 eps 0.778\n",
      "Ep 60 reward -6.0 MA(100) -3.62 eps 0.740\n",
      "Ep 70 reward -10.0 MA(100) -4.54 eps 0.704\n",
      "Ep 80 reward -8.0 MA(100) -5.10 eps 0.670\n",
      "Ep 90 reward -10.0 MA(100) -5.59 eps 0.637\n",
      "Ep 100 reward -10.0 MA(100) -5.85 eps 0.606\n",
      "q_table stats: min -5.444, mean -0.428, max 1.454\n",
      "Ep 110 reward -10.0 MA(100) -7.26 eps 0.576\n",
      "Ep 120 reward -10.0 MA(100) -7.92 eps 0.548\n",
      "Ep 130 reward -9.0 MA(100) -8.55 eps 0.521\n",
      "Ep 140 reward -3.0 MA(100) -8.61 eps 0.496\n",
      "Ep 150 reward -9.0 MA(100) -8.67 eps 0.471\n",
      "Ep 160 reward -9.0 MA(100) -8.55 eps 0.448\n",
      "Ep 170 reward -3.0 MA(100) -8.27 eps 0.427\n",
      "Ep 180 reward -9.0 MA(100) -7.91 eps 0.406\n",
      "Ep 190 reward -7.0 MA(100) -7.38 eps 0.386\n",
      "Ep 200 reward -9.0 MA(100) -6.86 eps 0.367\n",
      "q_table stats: min -12.540, mean -1.367, max 1.635\n",
      "Ep 210 reward -6.0 MA(100) -6.61 eps 0.349\n",
      "Ep 220 reward 13.0 MA(100) -5.46 eps 0.332\n",
      "Ep 230 reward 2.0 MA(100) -4.74 eps 0.316\n",
      "Ep 240 reward 9.0 MA(100) -4.43 eps 0.300\n",
      "Ep 250 reward 7.0 MA(100) -3.71 eps 0.286\n",
      "Ep 260 reward 4.0 MA(100) -3.11 eps 0.272\n",
      "Ep 270 reward 3.0 MA(100) -2.11 eps 0.258\n",
      "Ep 280 reward -2.0 MA(100) -1.43 eps 0.246\n",
      "Ep 290 reward 0.0 MA(100) -0.88 eps 0.234\n",
      "Ep 300 reward -2.0 MA(100) -0.31 eps 0.222\n",
      "q_table stats: min -15.516, mean -1.922, max 2.577\n",
      "Ep 310 reward 13.0 MA(100) 0.57 eps 0.211\n",
      "Ep 320 reward -2.0 MA(100) 0.88 eps 0.201\n",
      "Ep 330 reward 2.0 MA(100) 1.82 eps 0.191\n",
      "Ep 340 reward 108.0 MA(100) 11.65 eps 0.182\n",
      "Ep 350 reward 103.0 MA(100) 21.98 eps 0.173\n",
      "Ep 360 reward 96.0 MA(100) 32.15 eps 0.165\n",
      "Ep 370 reward 94.0 MA(100) 41.80 eps 0.157\n",
      "Ep 380 reward 100.0 MA(100) 51.67 eps 0.149\n",
      "Ep 390 reward 97.0 MA(100) 61.62 eps 0.142\n",
      "Ep 400 reward 94.0 MA(100) 71.20 eps 0.135\n",
      "q_table stats: min -14.817, mean 1.894, max 22.659\n",
      "Ep 410 reward 101.0 MA(100) 80.82 eps 0.128\n",
      "Ep 420 reward 102.0 MA(100) 90.13 eps 0.122\n",
      "Ep 430 reward 101.0 MA(100) 99.33 eps 0.116\n",
      "Ep 440 reward 103.0 MA(100) 99.96 eps 0.110\n",
      "Ep 450 reward 104.0 MA(100) 99.87 eps 0.105\n",
      "Ep 460 reward 103.0 MA(100) 100.28 eps 0.100\n",
      "Ep 470 reward 98.0 MA(100) 100.46 eps 0.095\n",
      "Ep 480 reward 107.0 MA(100) 100.61 eps 0.090\n",
      "Ep 490 reward 96.0 MA(100) 100.64 eps 0.086\n",
      "Ep 500 reward 101.0 MA(100) 101.07 eps 0.082\n",
      "q_table stats: min -14.200, mean 8.681, max 54.117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Andrija Lukic\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pygame\\pkgdata.py:27: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 510 reward 102.0 MA(100) 101.67 eps 0.078\n",
      "Ep 520 reward 106.0 MA(100) 102.15 eps 0.074\n",
      "Ep 530 reward 112.0 MA(100) 102.82 eps 0.070\n",
      "Ep 540 reward 100.0 MA(100) 103.19 eps 0.067\n",
      "Ep 550 reward 101.0 MA(100) 103.55 eps 0.063\n",
      "Ep 560 reward 113.0 MA(100) 104.25 eps 0.060\n",
      "Ep 570 reward 106.0 MA(100) 104.63 eps 0.057\n",
      "Ep 580 reward 105.0 MA(100) 105.40 eps 0.055\n",
      "Ep 590 reward 108.0 MA(100) 106.16 eps 0.052\n",
      "Ep 600 reward 104.0 MA(100) 106.92 eps 0.049\n",
      "q_table stats: min -13.698, mean 14.215, max 77.770\n",
      "Ep 610 reward 106.0 MA(100) 107.31 eps 0.047\n",
      "Ep 620 reward 104.0 MA(100) 108.05 eps 0.045\n",
      "Ep 630 reward 108.0 MA(100) 108.46 eps 0.043\n",
      "Ep 640 reward 106.0 MA(100) 108.86 eps 0.040\n",
      "Ep 650 reward 102.0 MA(100) 109.02 eps 0.038\n",
      "Ep 660 reward 110.0 MA(100) 109.19 eps 0.037\n",
      "Ep 670 reward 106.0 MA(100) 109.86 eps 0.035\n",
      "Ep 680 reward 113.0 MA(100) 110.03 eps 0.033\n",
      "Ep 690 reward 112.0 MA(100) 110.42 eps 0.031\n",
      "Ep 700 reward 114.0 MA(100) 110.68 eps 0.030\n",
      "q_table stats: min -13.506, mean 17.689, max 93.536\n",
      "Ep 710 reward 110.0 MA(100) 110.99 eps 0.028\n",
      "Ep 720 reward 108.0 MA(100) 111.11 eps 0.027\n",
      "Ep 730 reward 111.0 MA(100) 111.58 eps 0.026\n",
      "Ep 740 reward 120.0 MA(100) 112.16 eps 0.024\n",
      "Ep 750 reward 121.0 MA(100) 112.89 eps 0.023\n",
      "Ep 760 reward 112.0 MA(100) 113.08 eps 0.022\n",
      "Ep 770 reward 122.0 MA(100) 113.69 eps 0.021\n",
      "Ep 780 reward 126.0 MA(100) 114.68 eps 0.020\n",
      "Ep 790 reward 132.0 MA(100) 115.32 eps 0.019\n",
      "Ep 800 reward 132.0 MA(100) 116.18 eps 0.018\n",
      "q_table stats: min -13.421, mean 19.154, max 97.376\n",
      "Ep 810 reward 141.0 MA(100) 116.97 eps 0.017\n",
      "Ep 820 reward 126.0 MA(100) 118.12 eps 0.016\n",
      "Ep 830 reward 122.0 MA(100) 118.90 eps 0.016\n",
      "Ep 840 reward 120.0 MA(100) 119.49 eps 0.015\n",
      "Ep 850 reward 118.0 MA(100) 119.96 eps 0.014\n",
      "Ep 860 reward 129.0 MA(100) 121.26 eps 0.013\n",
      "Ep 870 reward 117.0 MA(100) 121.55 eps 0.013\n",
      "Ep 880 reward 119.0 MA(100) 121.50 eps 0.012\n",
      "Ep 890 reward 123.0 MA(100) 121.80 eps 0.012\n",
      "Ep 900 reward 120.0 MA(100) 122.26 eps 0.011\n",
      "q_table stats: min -13.386, mean 19.834, max 100.311\n",
      "Ep 910 reward 119.0 MA(100) 122.35 eps 0.010\n",
      "Ep 920 reward 118.0 MA(100) 122.59 eps 0.010\n",
      "Ep 930 reward 135.0 MA(100) 122.57 eps 0.010\n",
      "Ep 940 reward 134.0 MA(100) 123.43 eps 0.010\n",
      "Ep 950 reward 134.0 MA(100) 123.87 eps 0.010\n",
      "Ep 960 reward 128.0 MA(100) 123.39 eps 0.010\n",
      "Ep 970 reward 125.0 MA(100) 123.63 eps 0.010\n",
      "Ep 980 reward 132.0 MA(100) 124.67 eps 0.010\n",
      "Ep 990 reward 140.0 MA(100) 125.38 eps 0.010\n",
      "Ep 1000 reward 133.0 MA(100) 125.97 eps 0.010\n",
      "q_table stats: min -13.365, mean 19.884, max 98.542\n",
      "Ep 1010 reward 130.0 MA(100) 126.99 eps 0.010\n",
      "Ep 1020 reward 138.0 MA(100) 127.32 eps 0.010\n",
      "Ep 1030 reward 153.0 MA(100) 127.59 eps 0.010\n",
      "Ep 1040 reward 124.0 MA(100) 127.94 eps 0.010\n",
      "Ep 1050 reward 124.0 MA(100) 128.48 eps 0.010\n",
      "Ep 1060 reward 142.0 MA(100) 129.60 eps 0.010\n",
      "Ep 1070 reward 164.0 MA(100) 130.04 eps 0.010\n",
      "Ep 1080 reward 134.0 MA(100) 130.74 eps 0.010\n",
      "Ep 1090 reward 124.0 MA(100) 130.49 eps 0.010\n",
      "Ep 1100 reward 118.0 MA(100) 132.22 eps 0.010\n",
      "q_table stats: min -13.353, mean 19.878, max 96.359\n",
      "Ep 1110 reward 114.0 MA(100) 133.06 eps 0.010\n",
      "Ep 1120 reward 116.0 MA(100) 134.95 eps 0.010\n",
      "Ep 1130 reward 119.0 MA(100) 136.41 eps 0.010\n",
      "Ep 1140 reward 138.0 MA(100) 136.21 eps 0.010\n",
      "Ep 1150 reward 140.0 MA(100) 136.06 eps 0.010\n",
      "Ep 1160 reward 112.0 MA(100) 135.66 eps 0.010\n",
      "Ep 1170 reward 122.0 MA(100) 137.86 eps 0.010\n",
      "Ep 1180 reward 117.0 MA(100) 137.17 eps 0.010\n",
      "Ep 1190 reward 145.0 MA(100) 139.41 eps 0.010\n",
      "Ep 1200 reward 117.0 MA(100) 139.24 eps 0.010\n",
      "q_table stats: min -13.331, mean 19.952, max 93.466\n",
      "Ep 1210 reward 137.0 MA(100) 140.76 eps 0.010\n",
      "Ep 1220 reward 119.0 MA(100) 141.40 eps 0.010\n",
      "Ep 1230 reward 161.0 MA(100) 143.38 eps 0.010\n",
      "Ep 1240 reward 157.0 MA(100) 147.19 eps 0.010\n",
      "Ep 1250 reward 181.0 MA(100) 148.89 eps 0.010\n",
      "Ep 1260 reward 134.0 MA(100) 154.48 eps 0.010\n",
      "Ep 1270 reward 180.0 MA(100) 156.64 eps 0.010\n",
      "Ep 1280 reward 155.0 MA(100) 162.42 eps 0.010\n",
      "Ep 1290 reward 163.0 MA(100) 165.08 eps 0.010\n",
      "Ep 1300 reward 157.0 MA(100) 172.07 eps 0.010\n",
      "q_table stats: min -13.296, mean 19.860, max 91.708\n",
      "Ep 1310 reward 313.0 MA(100) 177.50 eps 0.010\n",
      "Ep 1320 reward 201.0 MA(100) 180.19 eps 0.010\n",
      "Ep 1330 reward 154.0 MA(100) 184.94 eps 0.010\n",
      "Ep 1340 reward 172.0 MA(100) 183.74 eps 0.010\n",
      "Ep 1350 reward 186.0 MA(100) 189.06 eps 0.010\n",
      "Ep 1360 reward 187.0 MA(100) 185.20 eps 0.010\n",
      "Ep 1370 reward 113.0 MA(100) 186.62 eps 0.010\n",
      "Ep 1380 reward 169.0 MA(100) 186.01 eps 0.010\n",
      "Ep 1390 reward 254.0 MA(100) 190.73 eps 0.010\n",
      "Ep 1400 reward 119.0 MA(100) 185.66 eps 0.010\n",
      "q_table stats: min -13.289, mean 19.509, max 90.404\n",
      "Ep 1410 reward 167.0 MA(100) 183.51 eps 0.010\n",
      "Ep 1420 reward 150.0 MA(100) 180.95 eps 0.010\n",
      "Ep 1430 reward 134.0 MA(100) 174.44 eps 0.010\n",
      "Ep 1440 reward 159.0 MA(100) 173.13 eps 0.010\n",
      "Ep 1450 reward 141.0 MA(100) 167.67 eps 0.010\n",
      "Ep 1460 reward 138.0 MA(100) 167.46 eps 0.010\n",
      "Ep 1470 reward 115.0 MA(100) 162.73 eps 0.010\n",
      "Ep 1480 reward 136.0 MA(100) 158.03 eps 0.010\n",
      "Ep 1490 reward 129.0 MA(100) 149.10 eps 0.010\n",
      "Ep 1500 reward 122.0 MA(100) 145.44 eps 0.010\n",
      "q_table stats: min -13.289, mean 19.401, max 92.505\n",
      "Ep 1510 reward -10.0 MA(100) 136.92 eps 0.010\n",
      "Ep 1520 reward -9.0 MA(100) 134.46 eps 0.010\n",
      "Ep 1530 reward 168.0 MA(100) 135.66 eps 0.010\n",
      "Ep 1540 reward 166.0 MA(100) 135.22 eps 0.010\n",
      "Ep 1550 reward 145.0 MA(100) 137.58 eps 0.010\n",
      "Ep 1560 reward 220.0 MA(100) 135.17 eps 0.010\n",
      "Ep 1570 reward 337.0 MA(100) 138.94 eps 0.010\n",
      "Ep 1580 reward 316.0 MA(100) 147.03 eps 0.010\n",
      "Ep 1590 reward 480.0 MA(100) 162.67 eps 0.010\n",
      "Ep 1600 reward 480.0 MA(100) 170.95 eps 0.010\n",
      "q_table stats: min -13.278, mean 18.823, max 92.240\n",
      "Ep 1610 reward 131.0 MA(100) 183.13 eps 0.010\n",
      "Ep 1620 reward 141.0 MA(100) 188.17 eps 0.010\n",
      "Ep 1630 reward 480.0 MA(100) 193.88 eps 0.010\n",
      "Ep 1640 reward 480.0 MA(100) 206.81 eps 0.010\n",
      "Ep 1650 reward 480.0 MA(100) 221.19 eps 0.010\n",
      "Ep 1660 reward 120.0 MA(100) 234.36 eps 0.010\n",
      "Ep 1670 reward 167.0 MA(100) 241.17 eps 0.010\n",
      "Ep 1680 reward 182.0 MA(100) 241.71 eps 0.010\n",
      "Ep 1690 reward 110.0 MA(100) 237.20 eps 0.010\n",
      "Ep 1700 reward 150.0 MA(100) 250.52 eps 0.010\n",
      "q_table stats: min -13.263, mean 18.691, max 93.652\n",
      "Ep 1710 reward 480.0 MA(100) 255.82 eps 0.010\n",
      "Ep 1720 reward 128.0 MA(100) 262.47 eps 0.010\n",
      "Ep 1730 reward 135.0 MA(100) 269.08 eps 0.010\n",
      "Ep 1740 reward 175.0 MA(100) 264.88 eps 0.010\n",
      "Ep 1750 reward 113.0 MA(100) 258.26 eps 0.010\n",
      "Ep 1760 reward 186.0 MA(100) 262.04 eps 0.010\n",
      "Ep 1770 reward 154.0 MA(100) 262.35 eps 0.010\n",
      "Ep 1780 reward 123.0 MA(100) 269.65 eps 0.010\n",
      "Ep 1790 reward 121.0 MA(100) 273.06 eps 0.010\n",
      "Ep 1800 reward 135.0 MA(100) 266.19 eps 0.010\n",
      "q_table stats: min -13.263, mean 18.408, max 94.203\n",
      "Ep 1810 reward 105.0 MA(100) 262.18 eps 0.010\n",
      "Ep 1820 reward 108.0 MA(100) 271.90 eps 0.010\n",
      "Ep 1830 reward 142.0 MA(100) 270.97 eps 0.010\n",
      "Ep 1840 reward 152.0 MA(100) 271.82 eps 0.010\n",
      "Ep 1850 reward 151.0 MA(100) 272.12 eps 0.010\n",
      "Ep 1860 reward 480.0 MA(100) 274.46 eps 0.010\n",
      "Ep 1870 reward 480.0 MA(100) 270.59 eps 0.010\n",
      "Ep 1880 reward 480.0 MA(100) 269.70 eps 0.010\n",
      "Ep 1890 reward 480.0 MA(100) 279.71 eps 0.010\n",
      "Ep 1900 reward 480.0 MA(100) 273.00 eps 0.010\n",
      "q_table stats: min -13.259, mean 18.143, max 95.000\n",
      "Ep 1910 reward 134.0 MA(100) 287.73 eps 0.010\n",
      "Ep 1920 reward 480.0 MA(100) 280.72 eps 0.010\n",
      "Ep 1930 reward 480.0 MA(100) 287.64 eps 0.010\n",
      "Ep 1940 reward 106.0 MA(100) 292.38 eps 0.010\n",
      "Ep 1950 reward 124.0 MA(100) 290.65 eps 0.010\n",
      "Ep 1960 reward 480.0 MA(100) 291.11 eps 0.010\n",
      "Ep 1970 reward 480.0 MA(100) 311.79 eps 0.010\n",
      "Ep 1980 reward 480.0 MA(100) 314.82 eps 0.010\n",
      "Ep 1990 reward 151.0 MA(100) 308.30 eps 0.010\n",
      "Ep 2000 reward 480.0 MA(100) 321.85 eps 0.010\n",
      "q_table stats: min -13.260, mean 18.444, max 95.864\n",
      "Ep 2010 reward 480.0 MA(100) 318.33 eps 0.010\n",
      "Ep 2020 reward 480.0 MA(100) 321.75 eps 0.010\n",
      "Ep 2030 reward 480.0 MA(100) 326.73 eps 0.010\n",
      "Ep 2040 reward 480.0 MA(100) 337.40 eps 0.010\n",
      "Ep 2050 reward 480.0 MA(100) 345.32 eps 0.010\n",
      "Ep 2060 reward 480.0 MA(100) 351.14 eps 0.010\n",
      "Ep 2070 reward 115.0 MA(100) 337.09 eps 0.010\n",
      "Ep 2080 reward 135.0 MA(100) 336.49 eps 0.010\n",
      "Ep 2090 reward 136.0 MA(100) 336.67 eps 0.010\n",
      "Ep 2100 reward 478.0 MA(100) 344.14 eps 0.010\n",
      "q_table stats: min -22.265, mean 18.491, max 97.291\n",
      "Ep 2110 reward 334.0 MA(100) 345.68 eps 0.010\n",
      "Ep 2120 reward 292.0 MA(100) 346.01 eps 0.010\n",
      "Ep 2130 reward 290.0 MA(100) 338.09 eps 0.010\n",
      "Ep 2140 reward 266.0 MA(100) 326.66 eps 0.010\n",
      "Ep 2150 reward 290.0 MA(100) 324.02 eps 0.010\n",
      "Ep 2160 reward 299.0 MA(100) 314.71 eps 0.010\n",
      "Ep 2170 reward 288.0 MA(100) 315.65 eps 0.010\n",
      "Ep 2180 reward 330.0 MA(100) 312.68 eps 0.010\n",
      "Ep 2190 reward 283.0 MA(100) 308.96 eps 0.010\n",
      "Ep 2200 reward 282.0 MA(100) 294.89 eps 0.010\n",
      "q_table stats: min -22.504, mean 19.590, max 103.101\n",
      "Ep 2210 reward 289.0 MA(100) 286.32 eps 0.010\n",
      "Ep 2220 reward 296.0 MA(100) 282.90 eps 0.010\n",
      "Ep 2230 reward 262.0 MA(100) 281.84 eps 0.010\n",
      "Ep 2240 reward 317.0 MA(100) 286.11 eps 0.010\n",
      "Ep 2250 reward 329.0 MA(100) 290.74 eps 0.010\n",
      "Ep 2260 reward 347.0 MA(100) 295.51 eps 0.010\n",
      "Ep 2270 reward 317.0 MA(100) 300.10 eps 0.010\n",
      "Ep 2280 reward 346.0 MA(100) 303.55 eps 0.010\n",
      "Ep 2290 reward 394.0 MA(100) 308.59 eps 0.010\n",
      "Ep 2300 reward 384.0 MA(100) 315.72 eps 0.010\n",
      "q_table stats: min -22.710, mean 20.516, max 103.825\n",
      "Ep 2310 reward 358.0 MA(100) 320.97 eps 0.010\n",
      "Ep 2320 reward 480.0 MA(100) 340.32 eps 0.010\n",
      "Ep 2330 reward 480.0 MA(100) 356.48 eps 0.010\n",
      "Ep 2340 reward 480.0 MA(100) 372.79 eps 0.010\n",
      "Ep 2350 reward 139.0 MA(100) 377.22 eps 0.010\n",
      "Ep 2360 reward 133.0 MA(100) 371.99 eps 0.010\n",
      "Ep 2370 reward 480.0 MA(100) 379.38 eps 0.010\n",
      "Ep 2380 reward 480.0 MA(100) 395.81 eps 0.010\n",
      "Ep 2390 reward 480.0 MA(100) 409.30 eps 0.010\n",
      "Ep 2400 reward 480.0 MA(100) 422.17 eps 0.010\n",
      "q_table stats: min -27.596, mean 21.090, max 109.650\n",
      "Ep 2410 reward 480.0 MA(100) 436.78 eps 0.010\n",
      "Ep 2420 reward 480.0 MA(100) 437.68 eps 0.010\n",
      "Ep 2430 reward 480.0 MA(100) 439.76 eps 0.010\n",
      "Ep 2440 reward 480.0 MA(100) 439.76 eps 0.010\n",
      "Ep 2450 reward 480.0 MA(100) 449.60 eps 0.010\n",
      "Ep 2460 reward 480.0 MA(100) 468.15 eps 0.010\n",
      "Ep 2470 reward 480.0 MA(100) 473.19 eps 0.010\n",
      "Ep 2480 reward 480.0 MA(100) 473.25 eps 0.010\n",
      "Ep 2490 reward 480.0 MA(100) 473.81 eps 0.010\n",
      "Ep 2500 reward 480.0 MA(100) 468.99 eps 0.010\n",
      "q_table stats: min -29.835, mean 21.799, max 115.241\n",
      "Ep 2510 reward 480.0 MA(100) 468.26 eps 0.010\n",
      "Ep 2520 reward 480.0 MA(100) 462.68 eps 0.010\n",
      "Ep 2530 reward 480.0 MA(100) 460.35 eps 0.010\n",
      "Ep 2540 reward 480.0 MA(100) 459.05 eps 0.010\n",
      "Ep 2550 reward 480.0 MA(100) 459.77 eps 0.010\n",
      "Ep 2560 reward 480.0 MA(100) 458.50 eps 0.010\n",
      "Ep 2570 reward 480.0 MA(100) 459.48 eps 0.010\n",
      "Ep 2580 reward 480.0 MA(100) 459.48 eps 0.010\n",
      "Ep 2590 reward 480.0 MA(100) 460.55 eps 0.010\n",
      "Ep 2600 reward 480.0 MA(100) 465.37 eps 0.010\n",
      "q_table stats: min -29.792, mean 21.975, max 109.979\n",
      "Ep 2610 reward 480.0 MA(100) 464.83 eps 0.010\n",
      "Ep 2620 reward 480.0 MA(100) 469.40 eps 0.010\n",
      "Ep 2630 reward 480.0 MA(100) 471.73 eps 0.010\n",
      "Ep 2640 reward 480.0 MA(100) 473.03 eps 0.010\n",
      "Ep 2650 reward 480.0 MA(100) 473.03 eps 0.010\n",
      "Solved at episode 2658 (moving avg 475.84)\n"
     ]
    }
   ],
   "source": [
    "agent = train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9680a9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(fql_agent, num_episodes=100, render=False, seed=None, max_steps=None, verbose=True):\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    env = gym.make(config.ENV_NAME)\n",
    "    max_steps = max_steps or getattr(config, \"MAX_STEPS_PER_EPISODE\", 1000)\n",
    "\n",
    "    prev_eps = getattr(fql_agent, \"epsilon\", None)\n",
    "    if prev_eps is not None:\n",
    "        fql_agent.epsilon = 0.0\n",
    "\n",
    "    episode_rewards = []\n",
    "    try:\n",
    "        for ep in range(1, num_episodes + 1):\n",
    "            if seed is not None:\n",
    "                obs, _ = env.reset(seed=seed + ep)\n",
    "            else:\n",
    "                obs, _ = env.reset()\n",
    "\n",
    "            total_reward = 0.0\n",
    "            terminated = truncated = False\n",
    "            steps = 0\n",
    "            cart_pos, cart_vel, pole_angle, pole_ang_vel = obs\n",
    "            action = fql_agent.get_initial_action([float(pole_angle), float(pole_ang_vel)])\n",
    "\n",
    "            while not (terminated or truncated) and steps < max_steps:\n",
    "                obs, reward, terminated, truncated, _ = env.step(int(action))\n",
    "                total_reward += reward\n",
    "                steps += 1\n",
    "\n",
    "                if terminated or truncated:\n",
    "                    break\n",
    "\n",
    "                _, _, pole_angle, pole_ang_vel = obs\n",
    "                action = fql_agent.get_action([float(pole_angle), float(pole_ang_vel)])\n",
    "\n",
    "            episode_rewards.append(total_reward)\n",
    "\n",
    "            if verbose and (ep == 1 or ep % 10 == 0 or ep == num_episodes):\n",
    "                recent_ma = float(np.mean(episode_rewards[-100:]))\n",
    "                print(f\"[Test] Ep {ep}/{num_episodes} reward {total_reward:.1f} recent_MA({min(100,len(episode_rewards))}) {recent_ma:.2f}\")\n",
    "\n",
    "    finally:\n",
    "        if prev_eps is not None:\n",
    "            fql_agent.epsilon = prev_eps\n",
    "        env.close()\n",
    "\n",
    "    stats = {\n",
    "        \"mean\": float(np.mean(episode_rewards)),\n",
    "        \"std\": float(np.std(episode_rewards)),\n",
    "        \"min\": float(np.min(episode_rewards)),\n",
    "        \"max\": float(np.max(episode_rewards)),\n",
    "        \"median\": float(np.median(episode_rewards)),\n",
    "    }\n",
    "\n",
    "    if verbose:\n",
    "        print(\"=== Evaluation summary ===\")\n",
    "        print(f\"episodes: {len(episode_rewards)}  mean: {stats['mean']:.2f}  std: {stats['std']:.2f}  min: {stats['min']:.1f}  max: {stats['max']:.1f}\")\n",
    "\n",
    "    return episode_rewards, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "567d7ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] Ep 1/50 reward 500.0 recent_MA(1) 500.00\n",
      "[Test] Ep 10/50 reward 500.0 recent_MA(10) 500.00\n",
      "[Test] Ep 20/50 reward 500.0 recent_MA(20) 500.00\n",
      "[Test] Ep 30/50 reward 500.0 recent_MA(30) 500.00\n",
      "[Test] Ep 40/50 reward 500.0 recent_MA(40) 499.43\n",
      "[Test] Ep 50/50 reward 500.0 recent_MA(50) 499.54\n",
      "=== Evaluation summary ===\n",
      "episodes: 50  mean: 499.54  std: 3.22  min: 477.0  max: 500.0\n"
     ]
    }
   ],
   "source": [
    "test_rewards, test_stats = test(agent[0], num_episodes=50, render=True, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b9bb905",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(f\"q_table_best_ep{2700}.npy\", agent[0].q_table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
